Understanding the challenges that are inherent to the building blocks of stream analytics is key to building a good solution. Therefore, this chapter provides background on characteristics and processing of event streams.





\section{Batch Processing}
For the better part of history, data was processed in form of batch datasets. An early analog example of batch data processing is the United States census: when the census was initiated in 1790, horseback riders recorded citizen counts per area and then transported their records to a central location for aggregation. While this is an extreme example, the principle still holds for digital data like periodic database dumps or bulk log transfers found in many batch processing systems today, where the the whole dataset is processed at once after arrival~\cite[p.~28]{Kreps.2014}.

A batch processing system takes a large amount of input data and runs a \textit{job} to process it. The produced result are often analytics, but arbitrary applications like search index building and machine learning feature extraction can be built with this method. Since batch jobs usually take a while to execute, they are not interactive but scheduled to run periodically. For example, web server logs can be imported once per day from the web server \glspl{node} and then user behavior analytics are available on the next morning. While latency is high, throughput, i.e. the amount of data processed per second, is a key performance metric since data volume is usually very large~\cite[p.~390]{Kleppmann.2017}.

As the volume of data grew, dataset became too large to be handled by a single \gls{node} (we use the term \textit{node} to refer to an individual server in a cluster). This sparked the development of distributed processing engines like Hadoop~\cite{Apache.2020} (based on the MapReduce~\cite{Dean.2008} programming model) and Spark~\cite{Apache.2020b}. These frameworks tackle two common challenges of large-scale batch processing~\cites[p.~429]{Kleppmann.2017}[pp.~362--373]{Akidau.2018}:
\begin{itemize}
	\item Scalability: support for distributed processing across nodes requires orchestration and \textit{partitioning}, i.e. the division of the dataset into subsets that can be processed in parallel, possibly on different \glspl{node}
	\item Fault-tolerance: guarantee of consistent and correct results even in case of job failures caused, for example, by hardware failure or scheduler-induced preemption
\end{itemize}
Having a framework to handle these issues makes focusing on the actual problem much easier.

Distributed batch processing engines assume that all functions applied to the data are stateless (no intermediate results are stored) and have no externally visible side effects (e.g., database updates)~\cite[p.~430]{Kleppmann.2017}. While these assumptions result in a deliberately restricted programming model, they facilitate distributed execution. Since no state needs to be shared between nodes, partition-based scalability is simple. In case of faults, the job can be restarted using the same input data, and the final output will be the same as if no faults had occurred (assuming deterministic operations). This is possible because input data are stored in a distributed and fault-tolerant file system like \gls{HDFS}~\cite{Apache.2020}. Therefore, the underlying file system facilitates processing across multiple nodes. Some processing engines store intermediate results to speed up re-computations after failures, but this often requires tracking of data ancestry or checkpointing~\cite[p.~430]{Kleppmann.2017}.

Batch processing has been successfully applied at massive scales, with Hadoop clusters at Yahoo of 35,000 nodes being used to store \SI{600}{\peta\byte} of data and run 34 million jobs every month~\cite{YahooDeveloperNetwork.2020}. However, it is only suitable for applications where low latencies are not required. Batch engines fall short when real-time processing is required, since they only process data once all input data are available. In practice, most data arrive as a continuous stream but are be divided into batches of a certain size for batch processing~\cite[p.~439]{Kleppmann.2017}. An obvious solution might be to decrease the batch size and run the job at a higher frequency, a technique known as \textit{micro-batching}. This can decrease the latency to less than a second, but ultra-low latency applications are still infeasible with micro-batch processing~\cite{Hazelcast.2019}. This is especially true when considering that data might arrive with a delay, which usually requires deferred processing or re-proccessing when late data arrive. Also, jobs that might span batch bounds, such as user session analysis in web applications, are inherently complex to implement~\cite[pp.~34--35]{Akidau.2018}.

Apart from the technical shortcomings, processing a continous stream of data in batches seems wrong from a philosohphical point of view. Batch processing frameworks are fundamentally ill-suited for this type of data. Why not build processing engines specifically designed with continuous streaming data in mind, that can overcome and embrace stream characteristics to enable new types of applications?

oceThis far-reaching sentiment was first expressed by Google researchers \citeauthor{TylerAkidau.2015} in \citeyear{TylerAkidau.2015}:
\begin{quote}
	We propose that a fundamental shift of approach is necessary to deal with these evolved requirements in modern data processing. We as a field must stop trying to groom unbounded datasets into finite pools of information that eventually become complete, and instead live and breathe under the assumption that we will never know if or when we have seen all of our data, only that new data will arrive [and] old data may be retracted~\cite[p.~1792]{TylerAkidau.2015}.
\end{quote}





\section{Stream Processing}
Stream-native processing, as opposed to batch processing on streams, comes with many challenges, but is ultimately the more powerful approach when dealing with streaming data. This section is an introduction to streams and stream processing, showing the fundamental characteristics and challenges.



\subsection{Streaming Data Properties}
The terms \enquote{stream processing} has been assigned a variety of meanings. Many associate low-latency, approximate, or speculative results with stream processing systems, especially in comparison to batch processing systems~\cite[pp.~23--24]{Akidau.2018}. While many historic systems had these properties, they are not inherent and should therefore not be used for definitions. Well-designed stream processing systems are perfectly capable of producing correct results. Therefore we use the definition of \citeauthor{Akidau.2018}:
\begin{quote}
	[A stream processing system is] a type of data processing engine that is designed with infinite datasets in mind~\cite[p.~24]{Akidau.2018}.
\end{quote}

Accordingly, a \textit{stream} is an \textit{unbounded} dataset that is infinite in size. Unboundedness means that a stream does not terminate and new data will arrive continuously. Therefore the dataset will never be complete Many data sources found in the real world produce data naturally as unbounded stream: sensors measurements, stock updates, user activities, credit card transactions, retail purchases, public transportation updates and business activities come from processes that are theoretically infinite (or at least very long-running), so we have to assume that they do not end. This is in contrast to \textit{bounded} datasets found in batch processing, which are regarded as complete.\footnote{This assumption can be made because there usually is a delay between data collection and data processing. Correct results can only be produced if this assumption holds and no data is late.}

The reason for the prevalence of batch processing despite the stream nature of most data stems from historical technical limitations of data collection~\cite[p.~29]{Kreps.2014}. Batch collection was the norm, be it for early census calculations or digital bulk dumps. Now we see a shift to more continuous data processing thanks to automation and digitization in the data collection process, which reduces latency but also requires new processing techniques. For the census example, this could mean to record births and deaths to produce continuous calculation counts.


\subsubsection{Time Domains}
A stream consist of \textit{records} that usually contains information about an \textit{event}, i.e. something of interest that happens in the real world. These might, for example, be purchases, website views, temperature changes or the arrival of a bus at a stop. When processing an event stream, two time domains are involved~\cite[p.~29]{Akidau.2018}:
\begin{itemize}
	\item Event time: the time at which the event actually occured in the real world
	\item Processing time: the time at which events are observed at a given processing stage
\end{itemize}

These two time domains often do not coincide. The processing time can never be before the event time. However, the delay between the occurance and processing of an event can be arbitrarily large. Usually, there is some small base delay due to, for example, network latencies and resource limitations. Other events might occasionally arrive later than expected, for example, when a vehicle broadcasting its position enters a tunnel or people using their phone sit in an airplane. In case historic data are processed, there might even be years of delay between event and processing time. Note that processing time is the natural order in which events arrive and are processed, processing by event time order requires additional effort.

The relationship of the two time domains can be visualized by plotting the progress of processing time over event time as shown in figure \ref{fig:background-timeskew-events}. Events (denoted by the diamonds) occur at event time and arrive at the system at processing time. The delay between these two is also known as \textit{event-time skew} or \textit{processing-time lag} (both terms are two perspectives on the same issue)~\cite[p.~30--31]{Akidau.2018}. The event-time skew for the green event is shown by the arrow. Events on the diagonal line would have no event-time skew. This would mean that data are processed instantly after occurring, which simplifies processing because events would arrive at the system in event time order. In reality, events are always above this line due to the base delay. However, the delay is not constant. While events occur every \SI{30}{\second} as shown in the top margin, some are observed much faster than others as shown in the right margin. In case of the green, blue and orange events, this even changes their order. This makes the stream (partially) unordered with respect to event time. Handling this skew and unorder is a key challenge stream processing frameworks have to solve~\cite[p.~28]{Akidau.2018}.

\begin{figure}
	\centering
	\includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}%
	\caption[Relationship between event time and processing time]{Relationship between event time and processing time: time skew varies a lot and leads to out-of-order arrival}
	\label{fig:background-timeskew-events}
\end{figure}



\subsection{Stream Processing Architectures}
The unbounded nature of streams, requiring continuous processing, cannot be handled by batch processing engines like Hadoop. While academic and commercial stream processing engines (SEEP, Naiad, Microsoft StreamInsight, IBM Streams) have existed before~\cite[p.~37]{Katsifodimos.2015}, Apache Storm was the first one to find widespread adoption when it was released in 2011~\cite[p.~375]{Akidau.2018}. Like MapReduce, it solves many of the common challenges like fault-tolerance, networking and serialization and allows developers to focus on solving the actual problem~\cite{Marz.2014}.

While Storm excelled at providing low-latency results, it did so by sacrificing features like exactly-once processing required for guaranteed correctness. This sparked the development of the Lambda architecture~\cite{Marz.2011}, shown in figure \ref{fig:background-lambda-architecture}. The batch layer produces correct results and handles fault-tolerance and scalability through the underlying processing engine, often Hadoop. Jobs are expressed in the MapReduce framework and store their results in a database optimized for batch writes and random reads for serving. The batch layer naturally lags behind real-time, therefore data is simultaneously processed in a real-time/speed layer, often implemented using Storm. The speed layer provides low-latency results but lacks in the correctness department due to approximative algorithms or possible system faults. This is acceptable, however, since speed layer results are overwritten by correct batch layer results once available. Even if a speed layer job fails, b batch layer results will be available at a later point. This requires the batch layer to store incoming data in an immutable and fault-tolerant way, also enabling recomputation in case processing code changes. By leveraging the two layers, the Lambda architecture provides low-latency, eventually-correct results~\cite[pp.~14--20,~pp.~27--28]{Marz.2015}.

\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
	FIGURE OF LAMBDA ARCHITECUTURE
	\caption[Lambda architecture]{Lambda architecture: the batch layer provides correct results, the speed layer provides low-latency results}
	\label{fig:background-lambda-architecture}
\end{figure}

While the Lambda architecture has been used to build many successful systems, it is inherently complex. The processing logic needs to be implemented twice and in both cases specifically engineered towards the processing engine. Even if the logic is implemented in a higher-level API that can be compiled to MapReduce and stream processing jobs, the twofold operational effort remains~\cite{Kreps.2014c}.

The Lambda architecture was born out of necessity since no framework could guarantee both low latency and correctness. However, more and more modern stream processing frameworks are able to provide the batch layer's correctness and the speed layer's correctness in a single system, much simplifying development and operations. This is called the Kappa architecture, shown in figure \ref{fig:background-kappa-architecture}. Instead of a storing data on a distributed file system, the stream is often stored in a replayable stream transport platform like Apache Kafka. This enables fault tolerance and recomputation in case of processing logic changes~\cite{Kreps.2014c}.

\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
	FIGURE OF KAPPA ARCHITECUTURE
	\caption[Kappa architecture]{Kappa architecture: a single processing engine provides correct, low-latency results}
	\label{fig:background-kappa-architecture}
\end{figure}

Spark Streaming~\cite{Apache.2020c} was the first large-scale stream processing engine being suited for use in a Kappa architecture. While not a true streaming but rather micro-batch processing engine, the latency was low enough for most applications. Since micro-batching uses batch processing under the hood, consistency and correct results were guaranteed. However, Spark Streaming lacked support for processing in event-time order, therefore producing correct results only in case of in-order data or event-time-agnostic computations. Correctness is absolutely required for stream processing engines to achieve parity with batch processing engines. Tools for reasoning about time, and especially event time, are essential for dealing with unbounded streams~\cite[pp.~27--28]{Akidau.2018}. Sophisticated time handling with high flexibility was explored in Google's company-internal MillWheel~\cite{TylerAkidau.2013} framework and Dataflow~\cite{TylerAkidau.2015} processing models. Apache Flink~\cite{Katsifodimos.2015} was the first open-source framework to incorporate the ideas into a high-throughput, low-latency stream processing engine that supports event-time processing and guarantees correctness.

Another contribution of Dataflow and Flink is the realization that batch and stream processing can be unified. Bounded batch datasets are effectively a section of an unbounded stream dataset, as shown in figure \ref{fig:background-bounded-unbounded-datasets}, and jobs can be specified using the same API and be executed on the same engine. However, bounded datasets are amenable to additional optimizations towards throughput at the cost of latency by increasing bundling sizes and computing processing stages successively instead of continuously~\cites[p.~35]{Katsifodimos.2015}[pp.~198--199]{Akidau.2018}. Such a unified processing engine decreases development and operations cost since code and infrastructure can be shared, and allows to balance latency and throughput based on the use case.

\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
	FIGURE OF BOUNDED AND UNBOUNDED DATASETS
	% https://flink.apache.org/img/bounded-unbounded.png
	\caption[Relationship between bounded and unbounded datasets]{Relationship between bounded and unbounded datasets: bounded datasets are sections of an unbounded dataset}
	\label{fig:background-bounded-unbounded-datasets}
\end{figure}



\subsection{Processing Patterns}
Streams requires processing patterns that support their unbounded nature. There are three major categories:~\cite[p.~35]{Akidau.2018}:
\begin{description}
	\item[Time-agnostic] When the processing logic is purely data-driven, ordering by time is irrelevant. This makes processing very simple because out-of-order records need not be accounted for, therefore this pattern is supported by even the most basic streaming systems. This includes record-by-record processing like filtering based on a record attribute but also inner joins, where a joined record is produced once the respective records from all input streams have been observed.\footnote{If many uncompleted joins are to be expected, a timeout-based garbage collection becomes necessary to limit memory requirements, introducing a time component.}
	\item[Approximation] Approximation algorithms like sketches for frequency distribution or distinct-value queries~\cite{Cormode.2011b} are optimized to handle large quantities of data by trading exact correctness for computational feasability, albeit usually within some error bounds. However, they are often complicated which makes it difficult to invent new ones. Furthermore, they usually work only in processing time, limiting their applicability.
	\item[Windowing] To handle the unboundedness and lack of completeness of streams, they can be chopped into bounded datasets known as \textit{windows}, which can be processed independently. For example, a stream can be divided into contiguous sections of \SI{1}{\minute}, and results like aggregations are computed per section. More complex, even arbitrary, windows are also possible. Note that this pattern includes many more time-based processing types that are not immediately obvious. For example, pattern recognition effectively builds a window for each stream record ending with the final record of the pattern, resulting in variable-length windows~\cite[p.~350]{Yu.2011}. Additionally, regular windows can be used to limit the records regarded for pattern matching and expire partial patterns to keep state size in check~\cite[p.~354]{Yu.2011}. Another example are outer joins, where a joined record can also be produced when the respective records have only been observed from some of the input streams. Outer joins on streams require a timeout after which a partial join should be produced, which effectively determines the window length, where the window contains all records that were regarded for a record's join.
\end{description}

The focus of the rest of this thesis will be on the windowing pattern, since it has unique challenges compared to time-agnostic and approximative processing. Specifically, correct windowing of out-of-order streams requires event-time awareness, and relating data within those window requires keeping consistent and fault-tolerant state. We will refer to this type of stream processing as \textit{stream analytics}, since time and state are required for producing sophisticated and valuable insights. We will now regard these two challenges in depth.



\subsection{Windowing}
Windowing is a key technique for enabling processing of unbounded datasets which inherently lack completeness. Each window of a stream is a finite chunk that is a complete dataset in itself. In this section, we will look at window types and how latency and correctness can be balanced for the use case at hand.

Windows can either be non-keyed (windows apply to the stream as a whole) or keyed (the stream is divided into subsets by key, e.g., per user, to which windows are applied individually). Three commonly found window types are shown in figure \ref{fig:background-window-types}~\cite[p.~1794]{Akidau.2015}:
\begin{description}
	\item[Fixed/tumbling] Fixed windows are defined by a fixed-length temporal window size. For example, a fixed window of \SI{10}{\minute} divides the stream into subsets of data from 12:00 to 12:10, then 12:10 to 12:20, continuing that way until the processing is stopped. Windows may either be \textit{aligned} or \textit{unaligned} across keys, depending on if the windows of different keys start at the same time or are staggered by an offset, which spreads window completion load more evenly across time.
	\item[Sliding/hopping]  Sliding windows are defined by a fixed window size and a fixed period. For example a sliding window of \SI{10}{\minute} starting every \SI{1}{\minute} divides the stream into subsets from 12:00 to 12:10, 12:01 to 12:11, so every record ends up in 10 windows. The window size is often an integer multiple of the period, and sliding windows can also be aligned or unaligned. Note that fixed windows are a a special case of sliding windows where size equals period.
	\item[Session] Session windows are defined by a timeout gap to capture periods of activity. For example, user activity analysis on a website during one sitting is a common use case for session windows. Session windows are defined per key and the length depends on the data involved, therefore they are inherently unaligned. Because the window length cannot be defined in advance, they are one area where the stream processing excels compared to batch processing. Since sessions may span multiple bounded batch datasets, the dataset must be treated as unbounded. Otherwise, complex stitching is required~\cite[p.~35]{Akidau.2018}.
\end{description}
Apart from these time-based window types, there are also tuple-based windows that contain a fixed number of records. However, they are essentially a form of time-based windowswith incrementing logical timestamps~\cite[p.~47]{Ahmed.2018} and will therefore not be regarded further here.

\begin{figure}
	\begin{subfigure}[c]{0.32\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_spike}%
		\subcaption{Fixed}
	\end{subfigure}
	\begin{subfigure}[c]{0.32\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_levelshift}%
		\subcaption{Sliding}
	\end{subfigure}
	\begin{subfigure}[c]{0.32\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_drift}%
		\subcaption{Session}
	\end{subfigure}
	\caption{Common window types}
	\label{fig:background-window-types}
\end{figure}

All windows can be defined in both time domains. When windowing by processing time, incoming data are buffered for the specified period and then processed, as shown in figure \ref{fig:background-window-domains-processing}. This is straigtforward because windows are complete as soon as the window time has passed, therefore there are no late data to handle. It works well for many monitoring scenarios, where insights about data as they are observed is desired. However, most use cases require processing of data in event-time order, but there might be an arbitrarily long delay between an event occuring and the event being processed, which changes the order of events. This can lead to incorrect results if not handled appropriately~\cite[p.~41]{Akidau.2018}. For example, when recognizing patterns, out-of-order data can result in matches that do not actually exist, and other matches might be missed. In billing applications, where correctness is paramount, quarterly reports might contain incorrect numbers if records end up in the wrong windows. Therefore, windowing by processing time is not sufficient in many cases.

Windowing in the event-time domain, as shown in figure \ref{fig:background-window-domains-event} requires ordering the out-of-order data to assign them to the correct window. This requires extra effort because event time is not the natural time domain of processing. On the one hand, data need to be buffered longer until the window is closed, therefore windows of the same size are open much longer in event time than in processing time. This demands more resources, but optimizations can be made to, for example, store aggregates incrementally. What is more challenging, however, is judging the completeness of a window. If the event-time skew can be arbitrarily long, it is non-trivial to judge when all data for a specific event-time window have been observed. This simplest approach is to delay processing for a fixed amount of time. For example, if data is usually not delayed for more than \SI{30}{\second}, we can reasonably assumed that all data for this window has arrived when we close the window \SI{35}{\second} after a record with the window end timestamp has been observed. However, this is essentially a tradeoff between latency and completeness (and by extension, correctness), since waiting longer necessarily increases latency but also increases the probability that no data is missed. This black-and-white tradeoff is far from satisfactory for many use cases. Therefore, the Dataflow~\cite{Akidau.2015} model introduced fine-grained control over window semantics to balance correctness, latency and cost.

\begin{figure}
	\begin{subfigure}[c]{0.49\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_spike}%
		\subcaption{Processing time}
		\label{fig:background-window-domains-processing}
	\end{subfigure}
	\begin{subfigure}[c]{0.49\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_levelshift}%
		\subcaption{Event time}
		\label{fig:background-window-domains-event}
	\end{subfigure}
	\caption{Windowing in different time domains}
	\label{fig:background-window-domains}
\end{figure}


\subsubsection{Windowing in Dataflow}
In the Dataflow, windowing is strictly event-time-based. However, processing-time windows are possible when assigning the arrival time as the event time. We will now look at the four aspects that enable a clear and flexible definition of windows. For a more detailed description, refer to~\cite[chapter~2]{Akidau.2018}

\begin{description}
	\item[Transformations] Transformations define what results are produced from the records in a window. This includes aggregations like summing and counting, training machine learning models or detecting anomalies. Depending on the specific transformation, individual records can either be accumulated and processed all at once when window results are \textit{materialized} (i.e. emitted and sent downstream for storage or further processing), or records can be aggregated eagerly to spread computation load more evenly and minimize state size.
	\item[Windowing] Windowing determines which records are grouped together based on some strategy. This includes fixed, sliding and session windows, but custom strategies are supported as well. Custom strategies (but also the built-in ones) consist of window assignment, which assigns records to one or more windows, and optional window merging, which allows merging of windows for window evolution as more data arrive. For example, window merging is required for session windows when a record arrives that connects two sessions which were before separated by the timeout gap~\cite[pp.~136--146]{Akidau.2018}.
	\item[Triggers] Where windowing determines the location of windows in event time, triggers specify when transformation results are materialized in processing time. This allows windows to be evaluated more than once, where each specific result of the window's transformation is referred to as \textit{pane}. There are two general types of triggers~\cite[p.~60]{Akidau.2018}:
	\begin{itemize}
		\item Repeated update triggers: these trigger window evaluation periodically, either after a specific count of records or at some processing-time frequency, such as every minute. The choice of period is primarily a tradeoff between latency and computation requirements.
		\item Completeness triggers: these trigger window evaluation when they believe that all data for the window has been observed, and therefore the window is complete.
	\end{itemize}
	Repeated update triggers show evolving results over time that converge towards correctness, but they do not indicate when correctness is achieved~\cite[p.~63]{Akidau.2018}. Therefore, completeness triggers may be more appropriate for use cases where correctness is important.
	\item[Output Mode] The output mode describes how different panes, i.e. subsequent evaluation results of a window, are related and refine previous results. Therefore, the choice is only relevant if windows are triggered multiple times. We will use the naming proposed in~\cite[p.~94]{Akidau.2018} instead of the original naming from the Dataflow paper for clarity. There are three types of output modes, with an example of two panes shown in table \ref{tab:background-output-modes}:
	\begin{itemize}
		\item Delta: upon triggering, the result is materialized and any stored state is discarded. Therefore, successive panes are independent of each other. For example, when summing input records, only the sum of all panes will yield the total sum for the window.
		\item Value: upon triggering, the result is materialized but stored state is retained. Therefore, successive panes build on each other's results. For example, when summing input records, each pane contains the total sum for the window so far.
		\item Value and retracting: upon triggering, the result is materialized and any stored state is discarded. Additionally, previous panes are explicitly retracted. For example, when summing input records, each pane contains two parts: the total sum for the window so far, and a retraction for the old sum.
	\end{itemize}
	The choice of output mode usually depends on the input expected by downstream consumers. Aggregating consumers might expect deltas, while databases that are updated with new data require values.
\end{description}

\begin{table}
	\newcommand\heading[1]{\textcolor{white}{\textbf{\textsf{#1}}}}
	\renewcommand{\arraystretch}{1.2}
	\centering
	\begin{tabularx}{\textwidth}{X l l l}
	\rowcolor{black} ~ & \heading{Delta~~~~~~~} & \heading{Value~~~~~~~} & \heading{Value and Retracting~~~~~~} \vspace{2pt} \\
	Pane 1: inputs=[3] & 3 & 3 & 3 \\
	Pane 2: inputs=[6, 1] & 7 & 10 & 10, -3 \\
	\textbf{Value of final pane} & 7 & 10 & 10 \\
	\textbf{Sum of all panes} & 10 & 13 & 10
	\end{tabularx}
	\caption{Example of windowing output modes}
	\label{tab:background-output-modes}
\end{table}

These four composable pieces provide flexible tools to balance correctness, latency and cost by adjusting trigger frequencies and output modes which also affects compute and memore requirements. Completeness triggers play an important role for correctness, but can be hard to implement, especially when event-time skew is highly variant. \textit{Watermarks} are an approach to indicating input completeness in the even-time domain~\cite[pp.~64--66]{Akidau.2018}. The watermark denotes the point in event time up to which the system believes all inputs with lower event timestamps have been observed. In other words, the watermark is an assertion that no more data with event timestamps earlier than the watermark will arrive. Completeness triggers can trigger window materialization once the watermark passes the window end in the belief that no more records will be assigned to that window. Note that watermarks must be monotonically increasing~\cite[p.~88]{Akidau.2018}.

Watermarks can be a strict guarantee or an educated guess of completeness. Perfect watermarks are possible when the system has full knowledge of all input data, for example, when assigning arrival times as event times. In some cases, the data source itself might produce watermarks. Late data, i.e. data with a timestamp earlier than the watermark that arrive past the watermark, will never occur. In most practical applications, only heuristic watermarks that approximate a perfect watermark based on the available information are possible. Heuristic watermarks can be generated by incorporating knowledge of ordering within partitions or file growth rates~\cite[p.~66]{Akidau.2018}, but also as percentile watermarks~\cite[pp.~106--108]{Akidau.2018} based on the event-time skew distribution, if known. This would, for example, enable watermarking after 99\% of all data are believed to have been observed, decreasing latency by ignoring stragglers. Another common strategy is by specifying a fixed bound for event-time skew, limiting the expected out-of-orderness. For example, the watermark could always lag \SI{10}{\second} behind the latest known timestamp if we know that the event-time skew will never exceed \SI{10}{\second}.

While watermarks are very useful to judge window completeness, they have two shortcomings~\cite[pp.~68--69]{Akidau.2018}. Watermarks may sometimes be too slow, which increases latency. This might either be the case because the data really have a high delay, or because the watermark generation overestimates the delay. On the other hand, heuristic watermarks might be too fast due to their approximate nature, in which case late data might arrive after the watermark. Therefore, watermark-based completeness triggers alone cannot provide both low-latency and correctness.

This motivates the use of multiple triggers per window. Early repeated update triggers compensate for watermarks being too slow by periodically providing early results which are incomplete. A single on-time trigger based on the watermark materializes results which the system believes to be correct. In case the heuristic watermark was too fast, late repeated update triggers refine results when late data arrive. Often, the late trigger fires for every late data record. Note that the output mode needs to be set appropriately when windows might be triggered more than once. This ensures that downstream consumers process multiple panes per window correctly.

Window state needs to be retained after the watermark when late triggers are enabled. Due to practical resource limitations, a maximum \textit{allowed lateness} in processing time must be specified. After a window is completed by a watermark, state is expired after the maximum allowed lateness. Any record that arrives later will be discarded. Since the value of data diminishes with time, trading off resource cost for data value is usually sensible



\subsection{State Consistency}
\cite[Chapter~7]{Akidau.2018}
Many applications, especially as complex ones as analytics require state
e.g. partial matches or intermediate results of aggregations

for batch: assumed that job can be restarted completely when it fails
for streaming: assume that data might not be replayable from beginning, correctness and efficiency require persistent state

exactly once guarantees for correctness, requires offset and replayable source (at least data since last checkpoint)
requires explicit state which is known to cluster and can be checkpointed
exactly once especially important when side effects are non-idempotent~\cite[Chapter~5]{Akidau.2018}



\subsection{Stream--Table Duality}
streams are data in motion
table are data at rest
can be converted
\cite[pp.~174--212]{Akidau.2018}





\section{Stream Transport}
message queue, often ephemeral
plain socket stream
pubsub



\subsection{Immutable logs}
append-only immutable log with persistency

Reasons~\cite[p.~31]{Kreps.2014}
\begin{itemize}
	\item flexible consumers, also for debugging
	\item ordering
	\item Buffering and isolation, e.g. for backpressure handling and replay on node failure, important prerequisite for robustness and correctness
\end{itemize}





\section{Event Processing}
not based on data shape/cardinality like batch or stream
rather data element type
however, often streams of events

event types and definitions
event type vs event instance

\subsection{Event Driven Architecture}
event happens in an instant
complex events are multiple events in correlated according to a pattern (have a duration)
composite event would be more fitting, but complex event is prevailing term

event driven types
event notification
event sourcing
event-carried state transfer

geoevents



\subsection{Pattern Recognition}
Also called Complex Event Processing, but ambiguous
pattern recognition performed on event streams
seit sql:2016 auch iso standard
not bound to stream processing, also e.g. microservices

selection of events to evaluate by window or consumption mode
