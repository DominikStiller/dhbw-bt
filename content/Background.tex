Understanding the challenges that are inherent to the building blocks of stream analytics is key to building a good solution. Therefore, this chapter provides background on characteristics and processing of event streams.





\section{Batch Processing}
For the better part of history, data was processed in form of \emph{\glspl{batch dataset}}. An early analog example of batch data processing is the United States census: when the census was initiated in 1790, horseback riders recorded citizen counts per area and then transported their records to a central location for aggregation. While this is an extreme example, the principle still holds for digital data like periodic database dumps or bulk log transfers found in many batch processing systems today, where the the whole dataset is processed at once after arrival~\cite[p.~28]{Kreps.2014}.

A batch processing system takes a large amount of input data and runs a \emph{\gls{job}} to process it. The produced result are often analytics, but arbitrary applications like search index building and machine learning feature extraction can be built with this method. Since batch jobs usually take a while to execute, they are not interactive but scheduled to run periodically. For example, web server logs can be imported once per day from the web server \glspl{node} and then user behavior analytics are available on the next morning. While latency is high, throughput, i.e. the amount of data processed per second, is a key performance metric since data volume is usually very large~\cite[p.~390]{Kleppmann.2017}.

As the volume of data grew, dataset became too large to be handled by a single \emph{\gls{node}}. This sparked the development of distributed processing engines\footnote{We use the terms \emph{processing engine}, \emph{processing framework} and \emph{processing platform} interchangeably since their semantic difference is not relevant for our purpose.} like Hadoop\footnote{\url{https://hadoop.apache.org/}} (based on the MapReduce~\cite{Dean.2008} programming model) and Spark\footnote{\url{https://spark.apache.org/}}. These frameworks tackle two common challenges of large-scale batch processing~\cites[p.~429]{Kleppmann.2017}[pp.~362--373]{Akidau.2018}:
\begin{itemize}
	\item Scalability: support for distributed processing across nodes requires orchestration and \emph{\glslink{dataset partition}{partitioning}}, i.e. the division of the dataset into subsets that can be processed in parallel, possibly on different \glspl{node}
	\item Fault tolerance: guarantee of consistent and correct results even in case of job failures caused, for example, by hardware failure or scheduler-induced preemption
\end{itemize}
Having a framework to handle these issues makes focusing on the actual problem much easier.

Distributed batch processing engines assume that all functions applied to the data are stateless (no intermediate results are stored) and have no externally visible side effects (e.g., database updates)~\cite[p.~430]{Kleppmann.2017}. While these assumptions result in a deliberately restricted programming model, they facilitate distributed execution. Since no state needs to be shared between nodes, partition-based scalability is simple. In case of faults, the job can be restarted using the same input data, and the final output will be the same as if no faults had occurred, assuming deterministic operations. This is possible because input data are stored in a distributed and fault-tolerant file system like \gls{HDFS}. Therefore, the underlying file system facilitates processing across multiple nodes. Some processing engines store intermediate results to speed up re-computations after failures, but this often requires tracking of data ancestry or checkpointing~\cite[p.~430]{Kleppmann.2017}.

Batch processing has been successfully applied at massive scales, with Hadoop clusters at Yahoo of 35,000 nodes being used to store \SI{600}{\peta\byte} of data and run 34 million jobs every month~\cite{YahooDeveloperNetwork.2020}. However, it is only suitable for applications where low latencies are not required. Batch engines fall short when real-time processing is required, since they only process data once all input data are available. In practice, most data arrive as a continuous stream but are be divided into batches of a certain size for batch processing~\cite[p.~439]{Kleppmann.2017}. An obvious solution might be to decrease the batch size and run the job at a higher frequency, a technique known as \emph{\gls{micro-batching}}. This can decrease the latency to less than a second, but ultra-low latency applications are still infeasible with micro-batch processing~\cite{Hazelcast.2020}. This is especially true when considering that data might arrive with a delay, which usually requires deferred processing or re-proccessing when late data arrive. Also, jobs that might span batch bounds, such as user session analysis in web applications, are inherently complex to implement~\cite[pp.~34--35]{Akidau.2018}.

Apart from the technical shortcomings, processing a continous stream of data in batches seems wrong from a philosohphical point of view. Batch processing frameworks are fundamentally ill-suited for this type of data. Why not build processing engines specifically designed with continuous streaming data in mind, that can overcome and embrace stream characteristics to enable new types of applications?





\section{Stream Processing}
Stream-native processing, as opposed to batch processing on streams, comes with many challenges, but is ultimately the more powerful approach when dealing with streaming data. It can enable many applications like real-time analysis of IoT sensor data, continuous credit card fraud and anomaly detection, live business process and quality monitoring, among many others. This section is an introduction to streams and stream processing, showing the fundamental characteristics and challenges.



\subsection{Streaming Data Properties}
The terms \enquote{stream processing} has been assigned a variety of meanings. Many associate low-latency, approximate, or speculative results with stream processing systems, especially in comparison to batch processing systems~\cite[pp.~23--24]{Akidau.2018}. While many historic systems had these properties, they are not inherent and should therefore not be used for definitions. Well-designed stream processing systems are perfectly capable of producing correct results. Therefore we use the definition of \citeauthor{Akidau.2018}:
\begin{quote}
	[A stream processing system is] a type of data processing engine that is designed with infinite datasets in mind~\cite[p.~24]{Akidau.2018}.
\end{quote}

Accordingly, a \emph{\gls{stream}} is an \emph{\glslink{stream}{unbounded}} dataset that is infinite in size. Unboundedness means that a stream does not terminate and new data will arrive continuously. Therefore the dataset will never be complete. Many data sources found in the real world produce data naturally as unbounded stream: sensors measurements, stock updates, user activities, credit card transactions, retail purchases, public transportation updates and business activities come from processes that are theoretically infinite (or at least very long-running), so we have to assume that they do not end. This is in contrast to \emph{\glslink{batch dataset}{bounded}} datasets found in batch processing, which are regarded as complete.\footnote{This assumption can be made because there usually is a delay between data collection and data processing. Correct results can only be produced if this assumption holds and no data is late.}

The reason for the prevalence of batch processing despite the stream nature of most data stems from historical technical limitations of data collection~\cite[p.~29]{Kreps.2014}. Batch collection was the norm, be it for early census calculations or digital bulk dumps. Now we see a shift to more continuous data processing thanks to automation and digitalization in the data collection process, which reduces latency but also requires new processing techniques. For the census example, this could mean to record births and deaths to produce continuous calculation counts.

Streams can also be regarded as \emph{\gls{data in motion}}. Scanning through the stream, it is possible to observe the evolution of data over time and build a view of the data at a single point in time. Such view are also called tables, which are \emph{\gls{data at rest}}. Relational databases have traditionally dealt with tables. Capturing the changes of a table in turn yields a stream. Therefore, streams and tables are really just two representations of the same data, a philosophy that many stream processing systems build upon~\cite[pp.~174--212]{Akidau.2018}. This concept is also known as stream--table duality~\cite{Sax.2018}.


\subsubsection{Time Domains}
A stream consist of \emph{\glspl{record}} that usually contain information about \glslink{primitive event}{events}. Events might, for example, be purchases, website views, temperature changes or the arrival of a bus at a stop. A stream emanates from a \emph{\glslink{stream producer}{producer}} and can be received by multiple \emph{\glslink{stream consumer}{consumers}}. When processing an event stream, two time domains are involved~\cite[p.~29]{Akidau.2018}:
\begin{itemize}
	\item \emph{\Gls{event time}}: the time at which the event actually occured in the real world
	\item \emph{\Gls{processing time}}: the time at which events are observed at a given processing stage
\end{itemize}

These two time domains often do not coincide. The processing time can never be before the event time. However, the delay between the occurance and processing of an event can be arbitrarily large. Usually, there is some small base delay due to, for example, network latencies and resource limitations. Other events might occasionally arrive later than expected, for example, when a vehicle broadcasting its position enters a tunnel or people using their phone sit in an airplane. In case historic data are processed, there might even be years of delay between event and processing time. Note that processing time is the natural order in which events arrive and are processed, processing by event time order requires additional effort.

The relationship of the two time domains can be visualized by plotting the progress of processing time over event time as shown in figure~\ref{fig:background-timeskew-events}. Events (denoted by the diamonds) occur in event time and arrive at the system in processing time. The delay between these two is also known as \emph{\gls{event-time skew}} or \emph{\gls{processing-time lag}} (both terms are two perspectives on the same issue)~\cite[p.~30--31]{Akidau.2018}. The event-time skew for the green event is shown by the arrow. Events on the diagonal line would have no event-time skew. This would mean that data are processed instantly after occurring, which simplifies processing because events would arrive at the system in event time order. In reality, events are always above this line due to the base delay. However, the delay is not constant. While events occur every \SI{30}{\second} as shown in the top margin, some are observed much faster than others as shown in the right margin. In case of the green, blue and orange events, this even changes their order. This makes the stream (partially) unordered with respect to event time. Handling this skew and unorder is a key challenge stream processing frameworks have to solve~\cite[p.~3]{Fragkoulis.2020}.

\begin{figure}
	\centering
	\includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}%
	\caption[Relationship between event time and processing time]{Relationship between event time and processing time: time skew varies a lot and leads to out-of-order arrival}
	\label{fig:background-timeskew-events}
\end{figure}



\subsection{Stream Processing Architectures}
The unbounded nature of streams, requiring continuous processing, cannot be handled by batch processing engines like Hadoop. While academic and commercial stream processing engines like SEEP, Naiad, Microsoft StreamInsight and IBM Streams have existed before~\cite[p.~37]{Carbone.2015}, Storm\footnote{\url{https://storm.apache.org/}} was the first one to find widespread adoption when it was released in 2011~\cite[p.~375]{Akidau.2018}. Like MapReduce, it solves many of the common challenges like fault tolerance, networking and serialization and allows developers to focus on solving the actual problem~\cite{Marz.2014}.

While Storm excelled at providing low-latency results, it did so by sacrificing features like exactly-once processing required for guaranteed correctness. This sparked the development of the Lambda architecture~\cite{Marz.2011}, shown in figure~\ref{fig:background-lambda-architecture}. The batch layer produces correct results and handles fault tolerance and scalability through the underlying processing engine, often Hadoop. Jobs are expressed in the MapReduce framework and store their results in a database optimized for batch writes and random reads for serving. The batch layer naturally lags behind real-time, therefore data is simultaneously processed in a real-time/speed layer, often implemented using Storm. The speed layer provides low-latency results but lacks in the correctness department due to approximative algorithms or possible system faults. This is acceptable, however, since speed layer results are overwritten by correct batch layer results once available. Even if a speed layer job fails, batch layer results will be available at a later point. This requires the batch layer to store incoming data in an immutable and fault-tolerant way, also enabling recomputation in case processing code changes. By leveraging the two layers, the Lambda architecture provides low-latency, eventually-correct results~\cite[pp.~14--20,~pp.~27--28]{Marz.2015}.

\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
	FIGURE OF LAMBDA ARCHITECUTURE
	\caption[Lambda architecture]{Lambda architecture: the batch layer provides correct results, the speed layer provides low-latency results}
	\label{fig:background-lambda-architecture}
\end{figure}

While the Lambda architecture has been used to build many successful systems, it is inherently complex. The processing logic needs to be implemented twice and in both cases specifically engineered towards the processing engine. Even if the logic is implemented in a higher-level API that can be compiled to MapReduce and stream processing jobs, the twofold operational effort remains~\cite{Kreps.2014c}.

The Lambda architecture was born out of necessity since no framework could guarantee both low latency and correctness. However, more and more modern stream processing frameworks are able to provide the batch layer's correctness and the speed layer's correctness in a single system, much simplifying development and operations. This is called the Kappa architecture, shown in figure~\ref{fig:background-kappa-architecture}. Instead of a storing data on a distributed file system, the stream is often stored in a stream transport platform like Kafka\footnote{\url{https://kafka.apache.org/}} that allows \emph{\gls{replay}}, i.e. rewinding to an earlier point in time and start reading from there instead of the latest record. This enables fault tolerance and recomputation in case of processing logic changes~\cite{Kreps.2014c}.

\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
	FIGURE OF KAPPA ARCHITECUTURE
	\caption[Kappa architecture]{Kappa architecture: a single processing engine provides correct, low-latency results}
	\label{fig:background-kappa-architecture}
\end{figure}

Spark Streaming\footnote{\url{https://spark.apache.org/streaming/}} was the first large-scale stream processing engine being suited for use in a Kappa architecture. While not a true streaming but rather micro-batch processing engine, the latency was low enough for most applications. Since micro-batching uses batch processing under the hood, consistency and correct results were guaranteed. However, Spark Streaming lacked support for processing in event-time order, therefore producing correct results only in case of in-order data or event-time-agnostic computations. Correctness is absolutely required for stream processing engines to achieve parity with batch processing engines. Tools for reasoning about time, and especially event time, are essential for dealing with unbounded streams~\cite[pp.~27--28]{Akidau.2018}. Sophisticated time handling with high flexibility was explored in Google's company-internal MillWheel~\cite{TylerAkidau.2013} framework and Dataflow~\cite{TylerAkidau.2015} processing models. Flink\footnote{\url{https://flink.apache.org/}} was the first open-source framework to incorporate the ideas into a high-throughput, low-latency stream processing engine that supports event-time processing and guarantees correctness.

Another contribution of Dataflow and Flink is the realization that batch and stream processing can be unified. Bounded batch datasets are effectively a section of an unbounded stream dataset, as shown in figure~\ref{fig:background-bounded-unbounded-datasets}, and jobs can be specified using the same API and be executed on the same engine. However, bounded datasets are amenable to additional optimizations towards throughput at the cost of latency by increasing bundling sizes and computing processing stages successively instead of continuously~\cites[p.~35]{Carbone.2015}[pp.~198--199]{Akidau.2018}. Such a unified processing engine decreases development and operations cost since code and infrastructure can be shared, and allows to balance latency and throughput based on the use case.

\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
	FIGURE OF BOUNDED AND UNBOUNDED DATASETS
	% https://flink.apache.org/img/bounded-unbounded.png
	\caption[Relationship between bounded and unbounded datasets]{Relationship between bounded and unbounded datasets: bounded datasets are sections of an unbounded dataset}
	\label{fig:background-bounded-unbounded-datasets}
\end{figure}

Stream processing jobs often consist of multiple stages that are connected into a directed \emph{\gls{dataflow graph}}\footnote{Note that dataflow graphs are a general concept that are also found in batch processing, and are different from the Dataflow stream processing model.}~\cite[p.~30]{Kreps.2014}. The graph starts at one or more stream sources and ends at one or more stream sinks. \emph{\Glspl{operator}} in between can filter, aggregate, join and split streams or transform them otherwise. Operators are essentially the graph's vertices, with streams between sources, operators and sinks forming the edges. A vertex further to the start of the graph (i.e. the sources) is referred to as \emph{\glslink{dataflow graph}{upstream}}, and a vertex further to the end of the graph (i.e. the sinks) is referred to as \emph{\glslink{dataflow graph}{downstream}}. An example graph is shown in figure~\ref{fig:background-dataflow-graph}. The graph can contain also contain loops to enable iterative algorithms like incremental machine learning and graph processing~\cite[p.~33]{Carbone.2015}. Dataflow graphs provide a very flexible programming model for building stream processing jobs, but also allow the processing framework to apply optimizations like operator fusion and fission~\cite{Hirzel.2014}. Stream processing jobs can also be written in a higher-level abstraction like streaming SQL and pattern matching specifications~\cite{Hirzel.2018}, which are eventually compiled down to dataflow graphs~\cite[p.30]{Carbone.2015}.

\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
	FIGURE OF DATAFLOW GRAPH
	% 3 sources, filter one, join, map, window and aggregate, sink
	% show flow direction and upstream/downstream
	% https://flink.apache.org/img/bounded-unbounded.png
	\caption[Example of a dataflow graph]{Example of a dataflow graph: three streams are joined, transformed and emitted as a single stream}
	\label{fig:background-dataflow-graph}
\end{figure}



\subsection{Processing Patterns}
Streams require processing patterns that support their unbounded nature. There are three major categories:~\cite[p.~35]{Akidau.2018}:
\begin{description}
	\item[Time-agnostic] When the processing logic is purely data-driven, ordering by time is irrelevant. This makes processing very simple because out-of-order records need not be accounted for, therefore this pattern is supported by even the most basic streaming systems. This includes record-by-record processing like filtering based on a record attribute but also inner joins, where a joined record is produced once the respective records from all input streams have been observed.\footnote{If many uncompleted joins are to be expected, a timeout-based garbage collection becomes necessary to limit memory requirements, introducing a time component.}
	\item[Approximation] Approximation algorithms like sketches for frequency distribution or distinct-value queries~\cite{Cormode.2011b} are optimized to handle large quantities of data by trading exact correctness for computational feasability, albeit usually within some error bounds. However, they are often complicated which makes it difficult to invent new ones. Furthermore, they usually work only in processing time, limiting their applicability.
	\item[Windowing] To handle the unboundedness and lack of completeness of streams, they can be chopped into bounded datasets known as \emph{\glspl{window}}, which can be processed independently. For example, a stream can be divided into contiguous sections of \SI{1}{\minute}, and results like aggregations are computed per section. More complex, even arbitrary, windows are also possible. Note that this pattern includes many more time-based processing types that are not immediately obvious. For example, pattern recognition effectively builds a window for each stream record ending with the final record of the pattern, resulting in variable-length windows~\cite[p.~350]{Adaikkalavan.2011}. Additionally, regular windows can be used to limit the records regarded for pattern matching and expire partial patterns to keep state size in check~\cite[p.~354]{Adaikkalavan.2011}. Another example are outer joins, where a joined record can also be produced when the respective records have only been observed from some of the input streams. Outer joins on streams require a timeout after which a partial join should be produced, which effectively determines the window length, where the window contains all records that were regarded for a record's join.
\end{description}

The focus of the rest of this thesis will be on the windowing pattern, since it has unique challenges compared to time-agnostic and approximative processing. Specifically, correct windowing of out-of-order streams requires event-time awareness, and relating data within those window requires keeping consistent and fault-tolerant state. We will refer to this type of stream processing as \emph{\gls{stream analytics}}, since time and state are required for producing sophisticated and valuable insights. Note that the term \enquote{stream analytics} is not well-defined in the literature and the boundary to other techniques like Complex Event Processing, Event Stream Processing, Distributed Stream Computing and Information Flow Processing is blurry~\cites{Dayarathna.2018}[p.~466]{Kleppmann.2017}. For our purpose, we consider any kind of sophisticated stream processing as stream analytics. We will now regard the challenges of time and state from a stream processing job developer's perspective. For an elaborate overview of specific implementations of out-of-order-data management and fault-tolerant state management in early and modern stream processing frameworks, refer to~\cite[chapters~3--5]{Fragkoulis.2020}.



\subsection{Windowing}
Windowing is a key technique for enabling processing of unbounded datasets which inherently lack completeness. Each window of a stream is a finite chunk that is a complete dataset in itself. In this section, we will look at window types and how latency and correctness can be balanced for the use case at hand.

Windows can either be non-keyed (windows apply to the stream as a whole) or keyed (the stream is divided into subsets by key, e.g., per user, to which windows are applied individually). Three commonly found window types are shown in figure~\ref{fig:background-window-types}~\cite[p.~1794]{TylerAkidau.2015}:
\begin{description}
	\item[Fixed/tumbling] Fixed windows are defined by a fixed-length temporal window size. For example, a fixed window of \SI{10}{\minute} divides the stream into subsets of data from 12:00 to 12:10, then 12:10 to 12:20, continuing that way until the processing is stopped. Windows may either be aligned or unaligned across keys, depending on if the windows of different keys start at the same time or are staggered by an offset, which spreads window completion load more evenly across time.
	\item[Sliding/hopping]  Sliding windows are defined by a fixed window size and a fixed period. For example a sliding window of \SI{10}{\minute} starting every \SI{1}{\minute} divides the stream into subsets from 12:00 to 12:10, 12:01 to 12:11, so every record ends up in 10 windows. The window size is often an integer multiple of the period, and sliding windows can also be aligned or unaligned. Note that fixed windows are a a special case of sliding windows where size equals period.
	\item[Session] Session windows are defined by a timeout gap to capture periods of activity. For example, user activity analysis on a website during one sitting is a common use case for session windows. Session windows are defined per key and the length depends on the data involved, therefore they are inherently unaligned. Because the window length cannot be defined in advance, they are one area where the stream processing excels compared to batch processing. Since sessions may span multiple bounded batch datasets, the dataset must be treated as unbounded. Otherwise, complex stitching is required~\cite[p.~35]{Akidau.2018}.
\end{description}
Apart from these time-based window types, there are also tuple-based windows that contain a fixed number of records. However, they are essentially a form of time-based windows with incrementing logical timestamps~\cite[p.~47]{Ahmed.2018} and will therefore not be regarded further here.

\begin{figure}
	\begin{subfigure}[c]{0.32\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_spike}%
		\subcaption{Fixed}
	\end{subfigure}
	\begin{subfigure}[c]{0.32\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_levelshift}%
		\subcaption{Sliding}
	\end{subfigure}
	\begin{subfigure}[c]{0.32\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_drift}%
		\subcaption{Session}
	\end{subfigure}
	\caption{Common window types}
	\label{fig:background-window-types}
\end{figure}

All windows can be defined in both time domains. When windowing by processing time, incoming data are buffered for the specified period and then processed, as shown in figure~\ref{fig:background-window-domains-processing}. This is straigtforward because windows are complete as soon as the window time has passed, therefore there are no late data to handle. It works well for many monitoring scenarios, where insights about data as they are observed is desired. However, most use cases require processing of data in event-time order, but there might be an arbitrarily long delay between an event occuring and the event being processed, which changes the order of events. This can lead to incorrect results if not handled appropriately~\cite[p.~41]{Akidau.2018}. For example, when recognizing patterns, out-of-order data can result in matches that do not actually exist, and other matches might be missed. In billing applications, where correctness is paramount, quarterly reports might contain incorrect numbers if records end up in the wrong windows. Therefore, windowing by processing time is not sufficient in many cases.

Windowing in the event-time domain, as shown in figure~\ref{fig:background-window-domains-event} requires ordering the out-of-order data to assign them to the correct window. This requires extra effort because event time is not the natural time domain of processing. On the one hand, data need to be buffered longer until the window is closed, therefore windows of the same size are open much longer in event time than in processing time. This demands more resources, but optimizations can be made to, for example, store aggregates incrementally. What is more challenging, however, is judging the completeness of a window. If the event-time skew can be arbitrarily long, it is non-trivial to judge when all data for a specific event-time window have been observed. This simplest approach is to delay processing for a fixed amount of time. For example, if data is usually not delayed for more than \SI{30}{\second}, we can reasonably assumed that all data for this window has arrived when we close the window \SI{35}{\second} after a record with the window end timestamp has been observed. However, this is essentially a tradeoff between latency and completeness (and by extension, correctness), since waiting longer necessarily increases latency but also increases the probability that no data is missed. This black-and-white tradeoff is far from satisfactory for many use cases. Therefore, the Dataflow~\cite{TylerAkidau.2015} model introduced fine-grained control over window semantics to balance correctness, latency and cost.

\begin{figure}
	\begin{subfigure}[c]{0.49\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_spike}%
		\subcaption{Processing time}
		\label{fig:background-window-domains-processing}
	\end{subfigure}
	\begin{subfigure}[c]{0.49\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_levelshift}%
		\subcaption{Event time}
		\label{fig:background-window-domains-event}
	\end{subfigure}
	\caption{Windowing in different time domains}
	\label{fig:background-window-domains}
\end{figure}


\subsubsection{Windowing in Dataflow}
In the Dataflow, windowing is strictly event-time-based. However, processing-time windows are possible when assigning the arrival time as the event time. We will now look at the four aspects that enable a clear and flexible definition of windows. For a more detailed description, refer to~\cite[chapter~2]{Akidau.2018}.

\begin{description}
	\item[Transformations] Transformations define what results are produced from the records in a window. This includes aggregations like summing and counting, training machine learning models or detecting anomalies. Depending on the specific transformation, individual records can either be accumulated and processed all at once when window results are \emph{\glslink{materialized result}{materialized}} (i.e. emitted and sent downstream for storage or further processing), or records can be aggregated eagerly to spread computation load more evenly and minimize state size. There are various choices for which event timestamp to assign the result, but often the end of the window is used~\cite[p.~101]{Akidau.2018}.
	\item[Windowing] Windowing determines which records are grouped together based on some strategy. This includes fixed, sliding and session windows, but custom strategies are supported as well. Custom strategies (but also the built-in ones) consist of window assignment, which assigns records to one or more windows, and optional window merging, which allows merging of windows for window evolution as more data arrive. For example, window merging is required for session windows when a record arrives that connects two sessions which were before separated by the timeout gap~\cite[pp.~136--146]{Akidau.2018}.
	\item[Triggers] Where windowing determines the location of windows in event time, triggers specify when transformation results are materialized in processing time. This allows windows to be evaluated more than once, where each specific result of the window's transformation is referred to as \emph{\glslink{window pane}{pane}}. There are two general types of triggers~\cite[p.~60]{Akidau.2018}:
	\begin{itemize}
		\item Repeated update triggers: these trigger window evaluation periodically, either after a specific count of records or at some processing-time frequency, such as every minute. The choice of period is primarily a tradeoff between latency and computation requirements.
		\item Completeness triggers: these trigger window evaluation when they believe that all data for the window has been observed, and therefore the window is complete.
	\end{itemize}
	Repeated update triggers show evolving results over time that converge towards correctness, but they do not indicate when correctness is achieved~\cite[p.~63]{Akidau.2018}. Therefore, completeness triggers may be more appropriate for use cases where correctness is important.
	\item[Output Mode] The output mode describes how different panes, i.e. subsequent evaluation results of a window, are related and refine previous results. Therefore, the choice is only relevant if windows are triggered multiple times. We will use the naming proposed in~\cite[p.~94]{Akidau.2018} instead of the original naming from the Dataflow paper for clarity. There are three types of output modes, with an example of two panes shown in table \ref{tab:background-output-modes}:
	\begin{itemize}
		\item Delta: upon triggering, the result is materialized and any stored state is discarded. Therefore, successive panes are independent of each other. For example, when summing input records, only the sum of all panes will yield the total sum for the window.
		\item Value: upon triggering, the result is materialized but stored state is retained. Therefore, successive panes build on each other's results. For example, when summing input records, each pane contains the total sum for the window so far.
		\item Value and retracting: upon triggering, the result is materialized and any stored state is discarded. Additionally, previous panes are explicitly retracted. For example, when summing input records, each pane contains two parts: the total sum for the window so far, and a retraction for the old sum.
	\end{itemize}
	The choice of output mode usually depends on the input expected by downstream consumers. Aggregating consumers might expect deltas, while databases that are updated with new data require values.
\end{description}

\begin{table}
	\newcommand\heading[1]{\textcolor{white}{\textbf{\textsf{#1}}}}
	\renewcommand{\arraystretch}{1.2}
	\centering
	\begin{tabularx}{\textwidth}{X l l l}
	\rowcolor{black} ~ & \heading{Delta~~~~~~~} & \heading{Value~~~~~~~} & \heading{Value and Retracting~~~~~~} \vspace{2pt} \\
	Pane 1: inputs=[3] & 3 & 3 & 3 \\
	Pane 2: inputs=[6, 1] & 7 & 10 & 10, -3 \\
	\textbf{Value of final pane} & 7 & 10 & 10 \\
	\textbf{Sum of all panes} & 10 & 13 & 10
	\end{tabularx}
	\caption{Example of windowing output modes}
	\label{tab:background-output-modes}
\end{table}

These four composable pieces provide flexible tools to balance correctness and latency by adjusting trigger frequencies and output modes, but also cost by affecting compute and memore requirements. Completeness triggers play an important role for correctness, but can be hard to implement, especially when event-time skew is highly variant. \emph{\Glspl{watermark}} are an approach to indicating input completeness in the even-time domain~\cite[pp.~64--66]{Akidau.2018}. The watermark denotes the point in event time up to which the system believes all inputs with lower event timestamps have been observed. In other words, the watermark is an assertion that no more data with event timestamps earlier than the watermark will arrive. Completeness triggers can trigger window materialization once the watermark passes the window end in the belief that no more records will be assigned to that window. Note that watermarks must be monotonically increasing~\cite[p.~88]{Akidau.2018}.

Watermarks can be a strict guarantee or an educated guess of completeness. Perfect watermarks are possible when the system has full knowledge of all input data, for example, when assigning arrival timestamps as event timestamps. In some cases, the data source itself might produce watermarks. \emph{\Gls{late data}}, i.e. data with an event timestamp earlier than the watermark that arrive past the watermark, will never occur. In most practical applications, only heuristic watermarks that approximate a perfect watermark based on the available information are possible. Heuristic watermarks can be generated by incorporating knowledge of the sources~\cite[p.~66]{Akidau.2018}, but also in form of percentile watermarks~\cite[pp.~106--108]{Akidau.2018} based on the event-time skew distribution, if known. This would, for example, enable watermarking after 99\% of all data are believed to have been observed, decreasing latency by ignoring stragglers. Another common strategy is by specifying a fixed bound for event-time skew, limiting the expected out-of-orderness. For example, the watermark could always lag \SI{10}{\second} behind the latest known timestamp if we know that the event-time skew will never exceed \SI{10}{\second}.

While watermarks are very useful to judge window completeness, they have two shortcomings~\cite[pp.~68--69]{Akidau.2018}. Watermarks may sometimes be too slow, which increases latency. This might either be the case because the data really have a high delay, or because the watermark generation overestimates the delay. On the other hand, heuristic watermarks might be too fast due to their approximate nature, in which case late data might arrive after the watermark. Therefore, watermark-based completeness triggers alone cannot provide both low-latency and correctness.

This motivates the use of multiple triggers per window. Early repeated update triggers compensate for watermarks being too slow by periodically providing early results which are incomplete. A single on-time trigger based on the watermark materializes results which the system believes to be correct. In case the heuristic watermark was too fast, late repeated update triggers refine results when late data arrive. Often, the late trigger fires for every late data record, which can drastically increase the number of computations in case of watermarks that are too fast by a large margin. Note that the output mode needs to be set appropriately when windows might be triggered more than once. This ensures that downstream consumers process multiple panes per window correctly.

Window state needs to be retained after the watermark when late triggers are enabled. Due to practical resource limitations, a maximum \emph{\gls{allowed lateness}} in processing time must be specified. After a window is completed by a watermark, state is expired after the maximum allowed lateness. Any record that arrives later will be discarded. Since the value of data diminishes with time, trading off resource cost for data value is usually sensible



\subsection{State Consistency and Persistence}\label{sec:background-state}
Any stream analytics solution that does not process streams record-by-record but correlates multiple records is stateful. For windowing, state consists of intermediate aggregation results. For pattern recognition, state consists of partial matches. For online machine learning training, the state consists of the current model parameters. Since stream processing jobs are effectively intended to run forever, interruptions like node failures, infrastructure maintenance or code changes are inevitable. To ensure correct results, the state needs to be persisted in a fault-tolerant way. This is especially important, since unbounded datasets usually cannot be replayed in their entirety, either because they are not retained forever, or because it is computationally infeasible~\cite[pp.~216--218]{Akidau.2018}. Simply storing state externally in a database can become a bottleneck~\cite[pp.~1718--1719]{Carbone.2017}. Compare this with batch processing, where it is often assumed that the dataset can be reprocessed in its entirety until the job succeeded.

Therefore, correct and efficient fault tolerance in stream processing requires persistent state that can be checkpointed. \emph{\Gls{checkpointing}} is the process of persisting in-memory state to a durable storage medium. This state needs to be exposed to the stream processing framework for management. To expose state, frameworks usually provide a flexible API with support for a variety of data structures~\cite[p.~228]{Akidau.2018}, often with efficient implementations of lists and maps~\cite[p.~1721]{Carbone.2017}. Apart from checkpointing for fault tolerance, this allows state redistribution during cluster scaling and alleviates the developer of needing to implement efficient persistence~\cite[pp.~1718--1719]{Carbone.2017}.

After fault recovery, the processing framework needs to guarantee that any materialized results are identical to the results if no fault occured. This is required for \emph{\gls{consistency}}~\cite[p.~15]{Fragkoulis.2020}. The key to consistent and correct results is \emph{\gls{exactly-once processing}}. This means that every stream record is guaranteed to be processed exactly once even in case of failures. For example, assume that a job counts the number of records in a stream, as shown in figure~\ref{fig:background-processing-semantics-example}. After having processed record 5, the job fails and needs to be restarted on another node. If the restarted job starts earlier than record 5, the total count will be higher than the actual count. This is referred to as \emph{\gls{at-least-once processing}}, since each record is guaranteed to be processed at least once. On the other hand, of the restarted job starts after record 5, the total count will be lower than the actual count. This is called \emph{\gls{at-most-once processing}}. Only if the job is guaranteed to be restarted from record 5 for exactly-once processing, the total count will be correct.

\begin{figure}
	\begin{subfigure}[c]{\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_spike}%
		\subcaption{Example scenario}
	\end{subfigure}
	\begin{subfigure}[c]{\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_spike}%
		\subcaption{At-least-once}
	\end{subfigure}
	\begin{subfigure}[c]{\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_levelshift}%
		\subcaption{At-most-once}
	\end{subfigure}
	\begin{subfigure}[c]{\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_levelshift}%
		\subcaption{Exactly-once}
	\end{subfigure}
	\caption{Example of different processing semantics after failure recovery}
	\label{fig:background-processing-semantics-example}
\end{figure}

Exactly-once processing needs to be end-to-end, which means that checkpointing not only needs to consider the processing framework but also sources and sinks~\cite[p.~153]{Akidau.2018}. A persistent and immutable source is required to be able to replay the stream from the last checkpointed position~\cite[p.~1722]{Carbone.2017}. If the stream is ephemeral, only at-most-once processing might be possible since the stream cannot be restarted from the correct position. The sink needs to be either idempotent (like most databases) or support transactions (like using two-phase commits)~\cite[pp.~1725--1726]{Carbone.2017}. To enable end-to-end exactly-once processing, the persistent state consists of the actual application state, but also needs to include the position in the stream on which that application state is based. Based on these information, a recovered job can provide consistent and correct results. However, exactly-once processing is not possible due to the nature of some sources and sink, and consistent and correct results cannot be guaranteed in those cases.

Most stream processing frameworks do not offer true exactly-once processing due to performance reasons, but rather \emph{\gls{effectively-once processing}}. This means that each record will only affect the results once, but might actually be processed multiple times in case of a job restart due to failure. This raises consistency issues, since non-deterministic operations might produce different results for the same inputs. For example, a database lookup for stream enrichment might return different data of a table has been updated in the meantime. Frameworks cope with non-determinism by checkpointing results from such transformations~\cite[pp.~155--156]{Akidau.2018} or simply assuming deterministic transformations~\cite[p.~1722]{Carbone.2017}.





\section{Stream Transport}
Having regarded streams as a concept, we now show how streams are physically manifested, stored and transported around. A \emph{\gls{stream transport}} system is responsible for moving streams between producers, processing systems and consumers. In its simplest form, a direct TCP connection can transport records from upstream producers to downstream consumers. For low-latency applications, UDP multicasting can be used. Brokerless messaging libraries like ZeroMQ\footnote{\url{https://zeromq.org/}} implement publish/subscribe messaging on top of these network protocols. Even HTTP and RPC requests can be used to push records from producers to consumers. However, such direct messaging methods fall short in terms of delivery guarantees and fault tolerance~\cite[pp.~441--441]{Kleppmann.2017}.

Message brokers like ActiveMQ\footnote{\url{http://activemq.apache.org/}}, Pulsar\footnote{\url{https://pulsar.apache.org/}} or RabbitMQ\footnote{\url{https://www.rabbitmq.com/}} provide a way to decouple producers and consumers using common protocols such as AMQP or JMS. The broker can store records durably to cope with offline or slow consumers by persisting them on disk until delivery is acknowledged. Many are also distributed for fault tolerance in case of broker node crashes. The decoupling makes record transport asynchronous, since producers do not know when a message is delivered to consumers~\cite[p.~443]{Kleppmann.2017}. Broker-centric instead of direct messaging furthermore allows multiple consumers to receive a stream without needing to change the producer, enabling organizational scalability and flexibility~\cite[pp.~20--22]{Kreps.2014} as well as debugging and monitoring by peeking into the stream~\cite[pp.~31--32]{Kreps.2014}. There are two patterns when multiple consumers receive the same stream, as illustrated in figure~\ref{fig:background-multiconsumer-patterns}~\cite[pp.~444--445]{Kleppmann.2017}. Load balancing means that each record is delivered to only one consumer, so processing load is spread over all consumers. Fan-out means that each record is delivered to all consumers, which allows for independent consumers to receive the full stream. Combinations of these two patterns using consumer groups are possible as well.

\begin{figure}
	\begin{subfigure}[c]{.32\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_spike}%
		\subcaption{Load balancing}
	\end{subfigure}
	\begin{subfigure}[c]{.32\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_spike}%
		\subcaption{Fan-out}
	\end{subfigure}
	\begin{subfigure}[c]{.32\textwidth}
		\centering
		% \includegraphics[width=0.95\textwidth]{plot_outlier_spike}%
		\subcaption{Mixed}
	\end{subfigure}
	\caption{Message distribution patterns with multiple consumers}
	\label{fig:background-multiconsumer-patterns}
\end{figure}

However, when using message brokers as transport platform in streaming systems, they have a number of shortcomings. First, they might deliver records more than once in case of lost acknowledgements, which might break exactly-once semantics or require deduplication in the processing framework. Secondly, most brokers do not make guarantees about record order, which requires special consideration in the processing framework. Thirdly, message brokers are ephemeral by design, which means that messages are discarded in the broker once delivered~\cite[pp.~445--446]{Kleppmann.2017}. This prevents stream replay for fault recovery or reprocessing after processing code changes.



\subsection{Immutable Logs}
Message brokers that use a log as data structure do not have the shortcomings of regular message brokers. A \emph{\gls{log}} is an append-only sequence where each record is assigned a monotonically increasing number called \emph{\gls{offset}}~\cite[pp.~1--3]{Kreps.2014}. Records are stored in the order they are appended and cannot be updated or deleted, therefore logs are immutable and are totally ordered. Log data structures are commonly found in databases for in form of write-ahead logs or replication logs, but also in many distributed consensus systems~\cite[pp.~54--66]{Kleppmann.2016}. When used for streaming, producers append new records to the end of the log, and consumers receive records by reading the log sequentially. Once a consumer reaches the end of the log, it waits for new records to be appended. Fan-out for multiple consumers is trivial since records are retained.

The total ordering solves the at-least-once delivery and out-of-orderness problems of traditional message brokers, since the offset enables consumers to know exactly which records they have or have not read yet. This offset can be part of state checkpointing as described when talking about state consistency in subsection~\ref{sec:background-state}. A configurable, possibly infinite, retention period instead of ephemeral records is feasible since performance does not degrade with increasing data size~\cite[p.~3]{Kreps.2011}. This enables stream replay which is required for exactly-once processing in case of faults. In fact, fault tolerance in many modern stream processing systems was only made possible by fundamentally relying on this ability of log-based sources~\cite[pp.~390--391]{Akidau.2018}. The ability to replay old data is also useful for reprocessing after bugfixes and for development and regression testing. While this capability is very common in batch processing, durable logs offer a robust and reliable streaming alternative~\cite[p.~390]{Akidau.2018}.

Durable log transportation platforms also provide an isolation layer between producers and consumers~\cites[pp.~450--450]{Kleppmann.2017}[p.~32]{Kreps.2014}. The log acts as large buffer that prevents consumers from being overwhelmed by a high volume of data by allowing consumers to read records at their own pace. Once the produced volume decreases, the consumer can catch up without needing to worry about data loss. This moves backpressure handling from the consumer to the transport layer.

A stream can be divided into \emph{\glslink{log partition}{partitions}} that contain the same record types but are otherwise separate logs that can be read and written to independently from other partitions~\cite[pp.~24--26]{Kreps.2014}. This is shown in figure~\ref{fig:background-log-partitions}. Partitions enable efficient distributed logs, since each partition can be hosted on a different node for horizontal scalability without synchronization overhead. Additionally, partitions can be replicated to other nodes for fault tolerance. While each partition is totally ordered within itself, there is no global order between partitions. In practice, this is only a minor limitation since processing jobs often handle partitions independently (e.g., in keyed windows). In that case, it makes sense to use the same key for determining the log partition, since a random partition is selected by default. Partitions also enable load balancing, since each partition can be assigned to a single consumer. In that respect, partitions are the smallest unit of parallelism~\cite[p.~4]{Kreps.2011}.

\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
	FIGURE OF PARTITIONS
	\caption[Log partitioning]{Log partitioning: each partition is an independent log with internal total order}
	\label{fig:background-log-partitions}
\end{figure}





\section{Event Processing}
So far, we have regarded streams of arbitrary records that have a timestamp. Usually, each record represents an event in the real-world. Many authors~\cites{Fragkoulis.2020}{Kleppmann.2019} go as far as treating stream records and events synonymously. Events are usually generated continously and therefore lend themselves well to be treated as unbounded stream, leveraging the processing and transport methods mentioned before. Thus, event processing can also be implemented as operator in a general-purpose stream processing framework instead of using a standalone event processing engine. However, the notion of events has been used long before stream processing frameworks gained popularity, for example in active database systems arising around 1990~\cite[p.~5]{Cugola.2012}. Therefore, we will regard events and event processing detached from stream processing in this section.

An \emph{\glslink{primitive event}{event}} is an occurence, something that has happened, within a particular system~\cite[p.~4]{Etzion.2011}. That system is often the real world, but may also be artificial like a simulation. Change events refer to significant changes of an environment, while status events refer to the observation of some value, even if it remained the same~\cite[p.~1]{Hinze.2009}. Events are always time-related and are therefore assigned a timestamp. Events can also have attributes that provide more detailed information like the value of a sensor readings. The attributes are defined in the \emph{\gls{event type}} which determines the event's semantic intent, and individual instances of an event type are called \emph{\glspl{event occurence}}~\cite[pp.~62--63]{Etzion.2011}. For example, events of type \enquote{environment measurement} might always have the attributes \enquote{temperature} and \enquote{humidity}, and instances of the event happening at different times have different values for these attributes. Events originating in the real world might have a geographical location as attribute, making the event a \emph{\gls{geo-event}}. \emph{\Glspl{complex event}}, also called composite events, emerge as result of relating multiple events in event processing, for example through windowed aggregation or pattern matching. In this context, regular events are also referred to as \emph{\gls{primitive event}}.

Processing of events comes in two flavors~\cite[pp.~10--11]{Etzion.2011}. \emph{\Gls{event-based programming}}, sometimes called \emph{\glslink{event-based programming}{event-driven architecture}}, uses events for interaction between components of a system. This asynchronous model provides better decoupling than synchronous request--response communication, since event producers have no expectation of event consumers' actions~\cite[pp.~33--34]{Etzion.2011}. This event-based communication, can be as simple as a notification, in which case the attribute size is rather small, often just an identifier which consumers can use to lookup further information. In other patterns like event-carried state transfer or event sourcing, that information is itself stored in an event attribute~\cite{Fowler.2017}. Event-driven architectures are described in-depth in \cite{Etzion.2011}.

In the other flavor, events are the subject of filtering, transformations and pattern recognition instead of a means of communication~\cite[pp.~121--127]{Etzion.2011}. This type of event processing is very similar, if not identical, to stream processing, and many concepts like event-time ordering and windowing apply to both in the same way. Therefore, treating stream records specifically as events instead of arbitrary objects that happen to be events is rather a semantical than technical difference. This thesis will focus on the second processing type, and we will approach event stream processing from the streaming side rather than the event side.

Pattern recognition over event streams emanates from the event domain, therefore it makes sense to regard stream records as events. \emph{\Gls{pattern recognition}} is the process of matching a stream of events against some pattern, and detecting instances of the pattern in the stream. This technique can be used to detect anomalies, monitor patients or stock tickers, or do predictive analytics. Pattern recognition is sometimes referred to as Complex Event Processing, but we will avoid this term due to its ambiguity in the literature. Processing of events in the order that they happened in is often crucial for correct pattern recognition. While standalone pattern recognition engines have existed for a long time, libraries built on top of stream processing frameworks can leverage their event-time ordering guarantees and allow pattern recognition to be easily integrated into stream processing jobs.

The \emph{\gls{event pattern}} is the template specifying one or more combinations of events to match the stream against. There is a wide range of pattern types that can be combined, but the most common include~\cite[pp.~219--236]{Etzion.2011}:
\begin{itemize}
	\item Logical expressions: \enquote{events of type A, B and C occured}
	\item Comparisons: \enquote{an event of type A with attribute x > 10 occured}
	\item Temporally ordered sequences: \enquote{first, an event of type A, then multiple events of type B, and finally an event of type C occured}
	\item Trends: \enquote{multiple events of type A occured with attribute x increasing monotonically}
	\item Spatial distance: \enquote{events of type A and B occured within \SI{5}{\kilo\meter} of each other}
	\item Spatial direction: \enquote{events of type A and B occured with A moving towards B}
\end{itemize}
The pattern recognition process is governed by policies that, for example, determine which events are selected for matching, whether events can participate in multiple matches and whether matches can overlap~\cite[pp.~237--242]{Etzion.2011}. The combination of patterns and policies provides rich tools to express a variety of situations and can therefore be applied widely.
