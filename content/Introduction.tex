the earlier insight arrive, the higher the value
historically, data arrived in batches, maybe once per day
this gave rise to batch processing systems, now highly performance optimized

\section{Problem}
lately, move from batch data to streaming data, since more data arrive continuously
wide application range

processing is treating streaming data like batch data
stream-native processing can improve correctness and stream-specific features (session windows)

non trivial because of time and State
time because events arrive out of order, state to enable complex tasks (pattern recognition)
still want to have correctness and faulttolerance at low latency
especially challenging at large scale

This far-reaching sentiment was first expressed by Google researchers \citeauthor{TylerAkidau.2015} in \citeyear{TylerAkidau.2015}:
\begin{quote}
	We propose that a fundamental shift of approach is necessary to deal with [the] evolved requirements in modern data processing. We as a field must stop trying to groom unbounded datasets into finite pools of information that eventually become complete, and instead live and breathe under the assumption that we will never know if or when we have seen all of our data, only that new data will arrive [and] old data may be retracted~\cite[p.~1792]{TylerAkidau.2015}.
\end{quote}

some existing platforms to solve problems
we want to understand stream-native processing platforms
get hands on experience with usecase


\section{Scope}
goal:
\begin{itemize}
    \item give an overview over correct, fault-tolerant, low-latency and scalable processing of streaming data
    \item demonstrate concepts through the design and implementation of an exemplary stream processing solution
\end{itemize}
