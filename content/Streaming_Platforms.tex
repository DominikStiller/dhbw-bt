After having regarded the theoretical foundations of streaming, we now look at existing implementations of stream transport and processing platforms that are available to the public (some companies have custom platforms for internal use only). We will present them under the aspects of correctness, fault tolerance, latency and scalability.





\section{Stream Transport Platforms}
Stream transport platforms are responsible for moving streams between producers and consumers, but may also store them for replay. Theoretically, any message queue or publish--subscribe message broker like ActiveMQ, Pulsar or RabbitMQ but also hosted services like Google Cloud Pub/Sub\footnote{\url{https://cloud.google.com/pubsub}} could be used for this task. However, these do not provide the end-to-end exactly-once\footnote{RabbitMQ has limited support for being used as exactly-once source but not as exactly-once sink since transactions are not supported~\cite{Apache.2020}. Pulsar can be used as exactly-once source using the same mechanism and as exactly-once sinks once transactions are implemented in Pulsar 2.7.0, therefore Pulsar could become a viable Kafka alternative for some applications in the future~\cite{StreamNative.2020}.} and ordering guarantees as well as stream retention required for correct and fault-tolerant stream processing. Log-based messaging systems, on the other hand, store stream records in order and can be used as exactly-once sources by including the record offset in checkpointed state. Kafka and Amazon Kinesis Data Streams\footnote{\url{https://aws.amazon.com/kinesis/data-streams/}} are two publicly available log-based stream transport platforms\footnote{The actual implementation of Kinesis Data Streams is unknown since it is a managed service, but because only appending is supported we regard it as log for all intents and purposes.}.

Kinesis Data Streams is a fully managed and reliable service hosted in the AWS cloud~\cite{AmazonWebServices.2020} and allows easy integration with other AWS offerings. The number of partitions per log, retention period and record encryption are the only configuration options. Up to \SI{1}{\mega\byte} or 1000 records can be produced to each partition per second, and up to \SI{2}{\mega\byte} can be consumed from each partition per second. Records can be retained for up to 7 days, which is sufficient to allow replay for fault tolerance in stream processing, but generally not enough for long-term retention to support data reprocessing in retrospect. While Kinesis Data Stream's operational costs are about four times lower than Kafka's, it lacks flexibility and throttles producers quickly~\cite{Nguyen.2018}.



\subsection{Apache Kafka}
Apache Kafka is a widely used~\cite{Apache.2020b} open-source stream transport platform first developed at LinkedIn. Kafka was the first system to leverage logs for high-throughput, low-latency messaging~\cite{Kreps.2011} and has been very influential on the design of modern stream processing platform by enabling fault-tolerance through replay~\cite[pp.~390--391]{Akidau.2018}. Records can be retained based on a time-based or size-based policy, with support for infinite retention which makes Kafka not only suitable for stream transport but also for persistent storage, effectively making it a file system for streams. This enables streams which show the evolution of data as primary data source, instead of traditional tables which represent data at a specific point in time\footnote{This approach leverages the stream--table duality to \enquote{turn the database inside out}~\cites{Kleppmann.2015}[pp.~459--462]{Kleppmann.2017}[pp.~174--212]{Akidau.2018}}. For example, the New York Times stores all historic and present articles and assets like images and tags in a single Kafka partition for total order, and every update to content does not overwrite old data but is appended to the log~\cite{Svingen.2017}. For serving, the log is replayed from the beginning and an index is built for efficient lookup of content. However, Kafka is usually used as continuous event streaming platform. For in-depth information on development with and operation of Kafka as well as internals, refer to~\cite{Narkhede.2017}.

Nodes in a Kafka cluster are called \emph{brokers} and serve as distributed storage of \emph{topics}. Topics are Kafka's notion of streams that contain a certain event type, and each topic consists of one or more partitions.  The logical position of a record in a partition is called the offset. Records are guaranteed to be stored in the order they are appended to the log within a single partition, but not across partition of a topic. The retention period can be configured per-topic, with records exceeding the retention period being removed from the beginning of the log. Logs can also be compacted, which means that only the most recent record for a key is retained. This can make sense to keep log size small if applications are only interested in the most recent record for a key. Partitions can be replicated across the cluster for fault tolerance. Each partition has a broker acting as partition leader, which clients connect to to consume existing records or append new records to a partition log, as shown in figure \ref{fig:platforms-kafka-cluster}. Follower replicas do not serve client requests but replicate records from the leader, and can be promoted to become the new partition leader in case the current leader crashes. Only in-sync replicas, i.e. follower replicas that have caught up with the current leader, are eligible for leader election. Cluster metadata like broker membership, partition locations and topic configurations are stored in Zookeeper\footnote{\url{https://zookeeper.apache.org/}}, a distributed and highly reliable configuration service required to run Kafka. However, this dependency will be removed in future releases to simplify deployment.


\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
	FIGURE OF KAFKA CLUSTER WITH PARTITIONS, LEADERS AND REPLICATION
	\caption[Structure of a Kafka cluster]{Structure of a Kafka cluster: partitions are spread across brokers acting as leader and follower replicas}
	\label{fig:platforms-kafka-cluster}
\end{figure}

Kafka supports flexible consumption patterns using \emph{consumer groups} previously shown in \ref{fig:background-multiconsumer-patterns}. Each partition of a topic is assigned to one consumer in a consumer group. Therefore, load balancing can be achieved when every consumer belongs to the same consumer group. On the other hand, if all consumers are part of different consumer groups, every consumer receives all records for fan-out. Mixed patterns are also possible by having multiple consumer groups with multiple consumers. When using the load balancing pattern, Kafka effectively acts like a traditional message queue, whereas fan-out is more similar to publish--subscribe semantics. Offsets between consumers of a consumer group are coordinated via special topics, which becomes necessary when consumers within a group change and partitions need to be rebalanced between consumers. Consumers commit their offsets regularly to avoid duplicate consumption. An important design decision is the use of consumer polling instead of broker pushing of records. This allows consumers to control the pace of consumption and not be overwhelmed, but also enables replay of older records. Kafka brokers use a zero-copy method when serving consumers, which means that Kafka sends records directly from disk to network without intermediate buffers, which is enabled by using the wire format for storage. Multiple brokers called bootstrap servers can be specified in the consumer for the initial connection to the cluster, after which the consumer can ask for the desired partition leader.

Producers also use bootstrap servers to connect to the partition leader for appending records to the log. Producers can control how many replicas must have received the record before the write can be considered successful. A producer may not wait for a reply from the partition leader at all to achieve very high throughput. A producer may wait for a success reply from the leader to be able to retry in case of leader failure. At the highest level of write safety but also the highest latency, a producer may wait for all in-sync replicas to have received the message. Producers can also batch multiple records in one write request to improve throughput at the cost of latency. The partition a new record will be assigned to is determined by a partitioner. The default partitioner assigns partitions based on a key like a user identifier or at random if none is given, but custom partitioning strategies are supported. Depending on the configuration, records sent from a single producer may or may not be appended to the partition in the same order. Events must be serialized before being stored as binary payload in a record. Kafka provides basic serializers, but often custom serializers based on JSON or a generic serialization library like Avro\footnote{\url{https://avro.apache.org/}}, Google Protocol Buffers\footnote{\url{https://developers.google.com/protocol-buffers}} (also known as \emph{protobuf}) or Thrift\footnote{\url{https://thrift.apache.org/}} is used. Kafka also supports transactions for atomic writes of multiple records across multiple partitions, which enables producers to implement exactly-once stream sinks. The prevent consumption of records of uncommitted transactions, consumers must support the isolation level directive.

Due to Kafka's popularity, a rich ecosystem of commercial offerings and libraries has emerged. Confluent, a company founded by one of the inventors of Kafka, provides a commercial Kafka platform with features like schema registries (centralized store for serialization schemas to be shared between consumers and producers) and tiered storage (offloading of old retained data from broker nodes to cheaper and scalable store like Amazon S3, also accelerating failure recovery and cluster rebalancing). Cloud platform providers like Amazon and Google also have fully managed offerings for Kafka, combining the flexibility and configurability of Kafka with the ease of operation of Kinesis Data Streams. Other tools like Kafka Connect allow the ingestion of streams from other systems to integrate Kafka into existing technology landscapes, and even more tools exist to mirror whole clusters, monitor performance and availability or run queries on streams.





\section{Stream Processing Platforms}
Stream processing platforms are also part of the stream transport platform ecosystem, consuming and producing streams while doing processing in between. They can also act as bridge between different transport platforms. The platform facilitates the development of processing jobs by taking care of aspects like event-time ordering, fault tolerance of state and scalability, and providing APIs to perform transformations on streams like windowing or pattern recognition. There is a number of frameworks with different approaches and paradigms that have been tried over time, refer to \cite{Fragkoulis.2020} for a comprehensive overview of the evolution and features of stream processing systems. We will focus on modern frameworks that can guarantee correct and consistent results through exactly-once processing. Therefore, we will not regard Samza\footnote{\url{http://samza.apache.org/}} (which only supports at-least-once processing), Spark Streaming (which has no support for event-time ordering and uses micro-batching) and Storm (although rudimentary stateful processing is supported with the Trident extension). However, they might still be suitable for certain applications. For example, Spark Streaming can leverage the extensive machine learning features of the Spark ecosystem. Kafka Streams\footnote{\url{https://kafka.apache.org/documentation/streams/}}, Spark Structured Streaming\footnote{\url{https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html}}, Hazelcast Jet\footnote{\url{https://jet-start.sh/}} and Flink are open-source stream processing frameworks which are suitable for stream analytics. All frameworks support high throughput and low latency, but exact numbers depend on the workload and configuration and will therefore not be compared here. Some performance results are presented in~\cite{Shahverdi.2019}. Note that all frameworks are under active development and new features are developed constantly, therefore existing limitations might be overcome.

Kafka Streams is a lightweight processing framework designed to run alongside the transport cluster, simplifying deployment because no separate cluster is needed but limiting it to Kafka as stream transport platform. Low-latency exactly-once processing with event-time ordering is supported. State can be stored in special compacted changelog topics for fault tolerance and queried from external applications. Kafka Streams also transparently handles rebalancing of partitions when adding and removing processing instances through consumer groups. Kafka Streams applications perform stateless or stateful transformations on stream and table abstractions as well as joining and windowing. While not having the flexibility of the Dataflow mode, windowing in Kafka streams supports all three common window types and early/late triggers. In general, Kafka Streams is suitable for many processing types but is limited to Kafka sources and sinks, and the simple deployment lacks flexibility.

Spark Structured Streaming is a stream processing API built on top of the Spark batch processing framework. Jobs can be submitted to regular Spark clusters and run there in a scalable and fault-tolerant fashion. Kafka and Kinesis Data Streams can be used as stream sources and sinks. Jobs can be run in two modes. Exactly-once processing can be guaranteeed in micro-batching mode, while only at-least-once processing is possible in continuous processing mode. However, latencies can only be as low as \SI{100}{\milli\second} in micro-batching mode whereas latencies of \SI{1}{\milli\second} can be achieved with continuous processing. Spark Structured Streaming supports all common stream operations, but uses the latest event timestamp as watermark, which can lead to incorrectly discarded data in case of varying event-time skew~\cite[p.~386]{Akidau.2018}. Spark Structured Streaming does not use a dataflow graph processing model but uses an incremental version of the static queries on an unbounded table from the batch API~\cite[p.~601]{Armbrust.2018}. Compared to other frameworks, Spark Structured Streaming has not many unique advantages. However, stream processing jobs can be run on an existing Spark cluster and existing developer know-how from Spark batch jobs can be applied to Spark Structured Streaming.

Hazelcast Jet is the the newest of the four frameworks, having been released in 2018 with focus on high performance and easy maintenance~\cite{Hazelcast.2017}. Hazelcast Jet is built on top of the Hazelcast In-Memory Data Grid to support distributed in-memory data structures and lookups for stream enrichment. Jobs can be run on a a cluster for scalability and fault tolerance. The framework comes with many connectors for sources and sinks, and supports exactly-once processing with Kafka. While it has no SQL-like API, common stream operations with event-time ordering are supported, including early triggers. It also integration with change data capture software for relational databases, and supports batch processing with a similar API to stream processing. Coroutines with shared threads instead of individual threads are used for executing tasks, maximizing CPU utilization and reducing context switching overhead. Hazelcast Jet is a promising stream processing engine, but still immature due to its young age.

While Beam\footnote{\url{https://beam.apache.org/}} is not a stream processing engine, it is still worth mentioning in this context. Beam is an abstraction layer that allows to write stream and batch processing jobs which can be executed on a number of stream execution frameworks, including Flink, Hazelcast Jet, Spark and Samza but also Google's fully managed Cloud Dataflow\footnote{\url{https://cloud.google.com/dataflow/}} platform. Beam is one implementation of the Dataflow paper~\cite{TylerAkidau.2015} and therefore provides an API for all windowing concepts presented earlier. While Beam was designed as portability layer for jobs, the exact features like exactly-once processing and window output modes are dependent on the underlying stream execution framework.



\subsection{Apache Flink}
Flink is one of the most popular distributed processing framework~\cite[p.~54]{Shahverdi.2019} for stream and batch jobs. It was the first open-source platform to support event-time ordering and guarantee exactly-once processing with consistent managed state while achieving high throughput and low latency~\cite[p.~37]{Carbone.2015}. Flink is successfully being used in production for search index building, online machine learning, real-time activity monitoring and alerting, dynamic ride pricing and continuous ETL~\cite{Apache.2020c}. There is also a number of fully managed and commercial offerings for Flink. The framework can connect to a number of sources and sinks, including Kafka, Kinesis Data Streams, RabbitMQ, ActiveMQ, Cassandra, Redis and HDFS. Refer to \cites{Carbone.2015}{Friedman.2016} and the official documentation for more detailed information on job development for Flink as well as its internals.

The Flink stack is shown in figure \ref{fig:platforms-flink-stack}. Flink can run on a single node, for example during development, but is usually distributed across nodes, possibly using a resource manager like YARN or Mesos, or containerized on Kubernetes. The runtime which executes dataflow graphs sits at the core of Flink. Dataflow graphs are specified using the DataSet API for batch processing or DataStream API for stream processing. Using the same engine for both bounded and unbounded data is possible since batch datasets are effectively special streams that are complete at the time of processing. however, the created dataflow graphs are optimized for either type. For example, operators in the graph are blocking when processing bounded datasets. The entire dataset is processed by the operator before forwarding the intermediate stream to the next operator. This stages scheduling increases latency but is more efficient than continuous processing which is necessary for streams. On top of these two APIs, Flink comes with domain-specific libraries for graph processing and pattern recognition, with a machine learning library being under active development. Additionally, operations known from relational databases on streams and bounded datasets are possible using the table API or SQL. These levels of abstraction allow the job developer to trade off between conciseness and expressiveness of the programming model, and open up stream processing to more users, possibly even non-experts who are only familiar with SQL.

\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
    FIGURE OF FLINK STACK
    % Figure 1 in Carbone.2015
	\caption[Components of Flink stack]{Components of Flink stack: multiple layers of abstraction run on top of diverse deployment environments}
	\label{fig:platforms-flink-stack}
\end{figure}

We will focus on operations on streams using the DataStream API. Common stream operations like \texttt{filter} and \texttt{map} but also splitting of a stream into multiple streams as well as joining of records of multiple streams based on some key are supported. Streams can be logically partitioned into keyed streams using the \texttt{keyBy} operation based on a key selector. All supported operators can be found in~\cite{Apache.2020d}. Arbitrary operations are possible using low-level process functions, which have access to stream records, can store fault-tolerant state, and can set timers in processing time and event time. This allows to implement transformations which are hard to implement using other operations. Streams can also be enriched with external data using asynchronous functions to, for example, make database requests, query external APIs, run machine learning inference or execute other operations that may take long. Asynchronous functions handle timeouts and stream ordering while keeping latency low.

Windowing is a common operation to divide the unbounded stream into bounded datasets. Flink's windowing is inspired by the Dataflow model. Records are assigned to windows using a window assigner. Out of the box, tumbling, sliding and session windows are supported, but implementing custom window assigners is possible. Every window is aligned with epoch by default, which means that, for example, hourly windows start on the hour instead of relative to the job start time. Records emitted from a window carry the window end timestamp, which allows windows to be chained. For example, chaining a \SI{10}{\minute} and a \SI{30}{\minute} introduces only \SI{30}{\minute} instead of \SI{40}{\minute} of latency. window Flink provides two types of transformations on the records of a window. Reducing and folding transformations like summing or averaging aggregate records eagerly, which reduces state size, especially for large windows. Arbitrary transformations are supported using process window functions, but require Flink to buffer all records for a window. Since a copy of each record is created for every window it belongs to, this can quickly lead to large states, for example when using a sliding window of 1 day with an evaluation period of 1 second. Flink supports completeness triggers, but only count-based triggers are supported for repeated updates. Custom triggers are supported to implement custom early/on-time/late triggers for balancing latency and correctness. Flink supports the delta and value output modes, but retractions must be implemented manually. Window evictors provide a way to remove certain records from the window before and after evaluation, which can for example be used for deduplication.

Processing data in the right time domain is crucial for correct results. Flink supports ordering of records by event time and processing time. The event timestamp can be extracted from records using a timestamp assigner, which also allows to use the ingestion timestamp (the time at which a record enters the Flink cluster) as event timestamp. Watermarks can be generated based on the event timestamps and emitted periodically (every \SI{200}{\milli\second} by default) or punctuated (when special records are observed). Flink ships with two periodic watermark generators. Watermarking for monotonously increasing timestamps assumes that records arrive in order with increasing timestamps. Watermarking with bounded out-of-orderness makes the watermark lag a fixed amount of event time behind the latest event timestamp. It is not recommended to emit watermarks for every record, since watermarks usually trigger downstream computations and would therefore degrade performance. Watermarks originate at sources in the dataflow graph, and are then propagated downstream. Simple operations like \texttt{map} simply forward watermark, while more complex operators like windows only forward watermarks after emitting the results of the window computation. Therefore, windowing adds to the latency of the watermark like it does to regular records. Join operators, which have multiple inputs, use the minimum of all incoming watermarks. To improve performance, event-time ordering is not guaranteed on a global level but only in operators that actually require the correct ordering.

A Flink cluster comprises a client, a job manager and one or more task managers, as shown in figure~\ref{fig:platforms-flink-cluster}. The client creates a dataflow graph from the job specification and submits it to the job manager after optimization. The job manager coordinates the distributed execution of the dataflow on the task managers. This includes tracking the state and progress of each operators and stream, but also coordinating checkpoints and recovery. While there is only one active job manager at any given time, failover is supported by promoting a standby job manager using a Zookeeper-based election. Task managers execute operators and report their status to the job manager. Every task manager offers a certain number of task slots (usually equal to the number of CPU cores) which determines how many operators can run in parallel on the task manager. Inter-task manager communication to exchange intermediate streams is handled through direct network connections, which contain mechanisms for flow control to propagate backpressure from operators to sources. The network stack is detailed in~\cite{Apache.2019} with considerations for trading off latency and throughput.

\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
    FIGURE OF FLINK ARCHITECTURE AND PROCESS MODELL
    % Figure 2 in Carbone.2015
	\caption[Cluster architecture and process model of Flink]{Cluster architecture and process model of Flink: jobs running on task managers are coordinated by a job manager}
	\label{fig:platforms-flink-cluster}
\end{figure}

While the dataflow graph shows the logical structure of the job, it is parallelized for actual execution. Operators are split into one or more parallel instances called \emph{subtasks}, and streams are split into one partition for each subtask. The degree of parallelism can be defined for each operator. Parallelization results in two types of inter-operator stream distribution patterns, as shown in figure~\ref{fig:platforms-flink-parallel-dataflow}. Redistribution is necessary in case two subsequent operators do not have the same number of subtasks, or streams need to be shuffled after a \texttt{keyBy} operation, since records for each key need to be processed by the same subtask. Subsequent operators with the same number of subtasks result in one-to-one streams and the stream partitioning can be preserved. Some subsequent operators like two $map$ transformations can also be chained into a slot sharing group. While usually every task slot is occupied by a single subtask, slot sharing groups allow multiple subtasks to be co-located within the same task slot to decrease traffic between subtasks for better performance

\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
    FIGURE OF PARALLEL DATAFLOW
    % https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/parallel_dataflow.svg
    % also show chaining
	\caption[Logical and physical view of parallel dataflow graph]{Logical and physical view of parallel dataflow graph: partitioning may require stream redistribution}
	\label{fig:platforms-flink-parallel-dataflow}
\end{figure}

Many operations like windowing and pattern recognition require state. Instead of storing the state in memory, state can be stored in the cluster so it can participate in checkpointing for fault tolerance. Flink provides interfaces to declare value, list and map states with optional time-to-live. State in operators can either be bound to a key or a whole subtask. Broadcast state is available to all subtasks and is often used to update rules for alerting or parameters for machine learning. State can also be queried from the outside. Flink supports three state backends, which determine where state is stored. The memory state backend stores state in the job manager's memory and is only recommended during development. The file system and RocksDB backends write state to disk. Since this state is stored locally and maintained in memory as well if possible, performance is usually not impacted much. RocksDB allows keeping very large state and is recommended for production.

State can be regularly \glslink{checkpointing}{checkpointed} for fault tolerance by taking a state snapshot (possibly incremental in case of RocksDB) and storing it to a durable medium like HDFS or Amazon S3, as shown in figure~\ref{fig:platforms-flink-checkpointing}. Flink uses barriers inserted into the regular stream to notify operators of checkpointing and dividing the stream into a part which will be included in the current snapshot and a part which will be snapshotted later. An efficient algorithm is used to align barriers in case of $join$ operators with multiple input streams. Snapshots also include input offsets of sources like Kafka to guarantee end-to-end exactly-once processing even after failure recovery. \emph{Savepoints} are a special kind of manually triggered checkpoint that can be used to stop and resume Flink jobs to perform code updates or maintenance, rescale by changing the parallelism, or fork the job for A/B testing or red/blue deployment.

\begin{figure}
	\centering
	% \includegraphics[width=0.55\textwidth]{plot_background_time_skew_events}
    FIGURE OF CHECKPOINTING
    % Combination of
    % https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/stream_aligning.svg
    % and
    % https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/stream_barriers.svg
	\caption[Logical and physical view of parallel dataflow graph]{Checkpointing mechanism of Flink: barriers trigger snapshots which are moved to a durable storage}
	\label{fig:platforms-flink-checkpointing}
\end{figure}

Flink provides a comprehensive set of features for correct, fault-tolerant, low-latency and scalable stream and batch processing. This makes it suitable for many use cases.
