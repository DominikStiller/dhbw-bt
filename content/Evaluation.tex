Finally, we want to evaluate our solution with the public transportation use case. Specifially, we want to observe the relationship of the latency and scalability aspects. We assume that results are correct and consistent even in case of failures according to Flink's guarantees (apart from at-least-once results due to the lack of transaction support in the visualization Kafka client). Latency effectively describes the real-time capabilities of our solution at a certain data volume. We define two types of end-to-end latencies:
\begin{itemize}
   \item Processing latency: the delay between an event being observed by the ingestion component and results based on that event arriving at the visualization component, can be divided into functional latency (e.g., due to buffering for windowing or joining, especially when using completeness triggers with watermarks) and non-functional latency (e.g., network shuffling and operator computations)
   \item Event latency: the delay between an event occurring and results based on that event arriving at the visualization component, consists of the processing latency plus the delay between the event's occurrence and ingestion
\end{itemize}
Note that the accuracy of the event latency is subject to time synchronization errors since we do not control the timestamping at the event source (i.e. the vehicles within the Helsinki public transportation system). This is not as much of an issue for the processing latency since we can synchronize the system clocks of all solution components using the same process. We will only regard the end-to-end processing latency, which comprises a major part of the event latency, and will therefore refer to it simply as \emph{end-to-end latency}. The other part was already described in \ref{fig:usecase-hsl-ingestion-lag} but is not under our control.

In this chapter, we only evaluate the performance of our public transportation analytics solution. For a more general performance evaluation and comparison of Flink with other stream processing platforms, refer to~\cites{Karimov.2018}{Shahverdi.2019}.





\section{Methodology}
To achieve the goal of analyzing the relationship between latency and data volume, we need to add two extra components to our solution. First, we need a means of measuring the processing and event latencies. Secondly, we need a way to increase volume without interfering with or distorting our results. Therefore, all data must be plausible and the number of keys must increase with the volume scaling factor to affect the number of partitions.



\subsection{Latency Tracking}
The job of the latency tracker is to track events within the solution and measure timestamps at different points. Based on the differences of these timestamps, the latencies can be calculated. Flink comes with an in-built latency tracker. However, it only measures non-functional latency and therefore does not reflect the true latency. Additionally, it only works within the Flink cluster, but our the latency we want to observe is end-to-end to include the ingestion and visualization components as well.

Therefore, we use our custom in-band latency tracking system. The ingestion component periodically injects special records called \emph{latency markers} into the regular stream which are received and evaluated by an additional latency tracker component. Latency markers are mock events with plausible data but special values to distinguish them from real events without needing to treat them separately in processing job. Therefore they experience the same latency as real events, including windowing, network shuffling and checkpointing latency, which can be significant if the state is large. The frequency of latency marker generation determines the upper bound for the resolution of the measurement.

\begin{figure}
	\centering
   % \includegraphics[width=\textwidth]{plot_hsl_daily_event_volume}
   FIGURE OF LATENCY TRACKING
	\caption[In-band latency tracking system]{In-band latency tracking system: the latency tracker tracks and timestamps latency markers}
	\label{fig:evaluation-latency-tracking}
\end{figure}

The flow of latency markers is shown in figure~\ref{fig:evaluation-latency-tracking}, with all timestamps marked $t_1$--$t_4$. The black arrows show the regular event flow which we presented in section~\ref{sec:usecase-dataflow}. The blue arrow is the end-to-end latency. It comprises several partial latencies between the individual components denoted by the green, red, and blue arrows. These can also be captured with our system, which allows us to better analyze where latencies come from. For example, when Flink slows down consumption due to backpressure, only the processing latency (the time between an event entering Flink and results based on it being produced by the job) increases. However, we cannot observe these partial latencies directly. While the ingestion and processing components add the ingestion timestamps just before producing the record to Kafka, we cannot measure the arrival time (i.e. the arrow tip) directly, since this would require modifying the existing components. Instead, we approximate these arrows by measuring the arrival time at the latency tracker. This assumes that the transport latency from Kafka to all downstream components is identical, i.e. records take the same time from Kafka to the processing and latency tracker components.

Each latency marker appears twice in the system, once as input record and once as analytics result record. The key challenge is to make latency markers unique identifiable between these two instances even when they are subject to windowing, which collapses multiple events into a single one. This is required to be able to track them end-to-end. To solve this problem, we must retain a one-to-one relationship between latency marker records and window results. Every window we use is keyed by either geocell or vehicle. Therefore, every latency marker has a unique geocell and vehicle number, so the latency marker will always be the only record in a window. Unique geocells are generated by spiraling outwards from an origin geocell. The $k$-th batch of geocells is the ring with distance $k$ from the origin cell. For example, the first batch of generated geocells are the direct neighbors of the origin cell, while the second batch are the neighbors of the first ring. H3 provides functions to easily generate these rings. We use \enquote{Point Nemo} as origin, the place on earth which is the farthest from any land. We can be relatively certain that no real events will originate from this point in the South Pacific. The vehicle number is generated randomly with a range outside of real numbers. Additionally, each latency marker uses a special value for the vehicle operator field. The event timestamp of latency markers is equal to the highest real event timestamp at any time. This ensures that latency markers experience the full functional latency introduced by watermarking.

To track latency markers generated every second in the ingestion component, the latency tracker component consumes all input and analytics topics. For every record, it checks if the record is a latency marker. For input records, checking if the event has the special operator is sufficient. Since the operator is not part of the analytics results records, checking if the geocell is within a certain distance of the generation origin cell is required for analytics result records. When a latency marker is first observed in an input topic, it is added to an internal cache with the geocell as key. The marker is retained for \SI{5}{\minute}, which sets an upper bound for the latency that can be measured. The latency tracker then stores two timestamps for the newly observed latency marker. The first is the ingestion timestamp ($t_1$) added by the ingestion component. The second one is the arrival timestamp at the latency tracker ($t_2$). Once the matching analytics result arrives, identified by the geocell, two more timestamps are added. After the ingestion timestamp ($t_3$) added by the processing component and the arrival timestamp ($t_4$) are recorded, all four timestamps are written to a CSV file for later analysis.

The latency tracker consumes all Kafka topics and partitions in a single thread. This means that it does not scale well and may measure wrong results under too much load. While we tried to parallelize the latency tracker by runnning multiple processes which each consume only one partition, the already observed latency markers must be shared between processes. Python does not have high-performance shared dictionaries, therefore this approach failed. The load on the latency tracker should be monitored to see if it may distort results.

Note that this approach is highly specific for our use case and does not generalize to all situations. We generate special markers for each input topic and also need to generate pseudo-patterns to produce latency marker results for the emergency stop detection jobs. Also, we do not support latency tracking for the final stop distribution job (since latency markers cannot be used for querying the routing API) and the flow direction job (since latency markers are limited to a single geocell). While these limitations can be overcome, applying our approach to other use cases must guarantee that unique end-to-end tracking of latency markers is possible.




\subsection{Volume Scaling}
The second component required for our evaluation can increase the volume of data, or throughput, that is being ingested into our system and needs to be processed. However, it should not distort the evaluation results, i.e. the results should show the same behavior as if the scaled data volume was ingested naturally from the HFP API. For example, the number of keys influences the number of partitions and network shuffling overhead, therefore there must be 10 times as many keys if the volume is scaled by a factor of 10. Since this scaling while maintaining the distribution of data is use case specific, we are using a custome volume scaler.

The volume scaler is integrated in the ingestion component. Instead of using the real-time HFP stream in the MQTT connector, we use the replay connector which replays a recording of MQTT messages from the actual API. Using the predictability of a recording allows us to repeat and compare results. It also allows us to replay the recording multiple times simultaneously to increase the data volume. To scale the volume by a factor of $n$, the ingestion component starts $n$ processes of the replay connector, as shown in figure \ref{fig:evaluation-volume-scaling}. Each process consists of one thread. The first thread reads and decompresses the recording file stored in S3, and parses it to retrieve the original MQTT messages. It then schedules the processing of the message in the second thread with the same original timing. For example, if two messages were received \SI{10}{\milli\second} apart during recording, these messages will be processed with the same time offset. The recorded MQTT message is then processed using the same processor as regular messages to extract the event timestamp and details into the appropriate protobuf message.

\begin{figure}
	\centering
   % \includegraphics[width=\textwidth]{plot_hsl_daily_event_volume}
   FIGURE OF VOLUME SCALING
	\caption[Volume scaling system]{Volume scaling system: each process replays the recording with an offset}
	\label{fig:evaluation-volume-scaling}
\end{figure}

Before producing the event to Kafka, the details need to be adjusted to prevent distorted results. Therefore, we change the unique identifier of each vehicle to increase the number of keys. We do not adjust the geocells, since these would stay the same if the higher volume occurred naturally. We also offset each individual replay to naturally diversify the ingested events. For example, the first replay starts from the very beginning of the recording, while the second replay starts 1000 records later, and the third replay 2000 records later. We then need to adjust the event timestamp so simultaneously ingested events have approximately the same event timestamp. We do so by setting the event timestamp of all replays to be within \SI{0.1}{\second} of the latest event timestamp in the first replay.

This latest timestamp is the only shared state between replay processes. However, this makes it hard to distribute the ingestion component across different nodes. Usually, every process runs on a single CPU core. Therefore, the possible scaling factor scales linearly with the number of cores but is also limited by it. If there are too many processes running, the desired volume is not achieved, records are not produced as quickly as they should be, and therefore the observed event time is slower than it actually was. Then, the ingestion component instead of the processing component becomes the bottleneck of the solution. This needs to be prevented during evaluation by ensuring that the ingestion rate is higher than the consumption rate of the Flink job~\cite[p.~73]{Lu.2014}. This can be achieved by monitoring the volume within the Kafka topics for plausibility.



\subsection{Setup}
Due to the limitations of the latency tracker, we only test the vehicle distribution, delay distribution and both emergency stop detection jobs. We run all jobs at once on the same cluster, and measure the latencies with different scaling factors between 1 and 64. The normal volume is about 700 events per second. While we only run the experiment once for each factor, we have observed the same behaviors in previous experiments with different setups, therefore the results can be assumed to be repeatable and not coincidencental.

We are using the following setup on AWS for evaluation designed to make the task managers the bottleneck:
\begin{itemize}
   \item Kafka: 4\texttimes m5.xlarge with Kafka 2.5.0, \SI{15}{\minute} retention, 4 partitions per topic with replication factor 3
   \item Flink Job Manager: 1\texttimes t3.medium with Flink 1.11.1, \SI{2}{\giga\byte} memory per job manager
   \item Flink Task Manager: 4\texttimes t3.medium with Flink 1.11.1, \SI{2}{\giga\byte} memory per task manager, memory state backend
   \item Ingestion: 1\texttimes c5.24xlarge
   \item Latency tracker: 1\texttimes m5.xlarge
\end{itemize}
All nodes use CentOS 7.6 with Linux kernel 3.10.0-957, Python 3.8.3 and OpenJDK 11.0.8. We deliberately choose small instances for Flink task managers to see the effects of scaling quicker. The ingestion instance has many cores to support large scaling factors. We run all jobs with a parallelism of 2, so each source task consumes two Kafka partitions. Every task manager has two task slots (one per vCPU), but still one job is usually executed on two different nodes and therefore requires network shuffling. We do not use checkpointing since the effect on performance strongly depends on its configuration. All jobs use event-time processing with \SI{1}{\second} bounded out-of-orderness watermarking. Note that this setup is for evaluation only. During regular operation, there would be no latency tracker, better task manager instances and a less powerful ingestion instance.





\section{Results}
The end-to-end latency of the vehicle distribution job over a span of \SI{30}{\second} without volume scaling is shown in figure~\ref{fig:evaluation-latency-vehicle-distribution-1x-short}. The buffering of records for windowing manifests itself in the sawtooth pattern with a period of \SI{5}{\second}, equal to the period of the sliding window. This adds functional latency for most records, additionally to the baseline latency of \SI{1}{\second} due to bounded-out-of-orderness watermarking (latency markers always have the event timestamp of the latest ingested event and therefore are buffered until an event with a timestamp one second in the future is observed). Most of the latency therefore comes from the processing latency within Flink (about 99.7\% of the total end-to-end latency), and the other partial latencies consisting mostly of transport within Kafka topics is negligible. Note that the emergency stop detection jobs are only affected by the watermarking, therefore they do not show the same windowing effects, as can be seen in figure~\ref{fig:appendix-latency-all-1x}. However, the results of these jobs are implausible when looking at higher scaling factors since the latency falls below \SI{1}{\second}, which should not happen with event-time processing.

\begin{figure}
   \centering
   \begin{subfigure}[c]{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{plot_latency_vehicle_distribution_1x_zoomed}%
      \subcaption{Short period}
      \label{fig:evaluation-latency-vehicle-distribution-1x-short}%
   \end{subfigure}
   \begin{subfigure}[c]{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{plot_latency_vehicle_distribution_1x_normal}%
      \subcaption{Long period}
      \label{fig:evaluation-latency-vehicle-distribution-1x-long}%
   \end{subfigure}%
   \caption{End-to-end latency for vehicle distribution job at normal volume}
   \label{fig:evaluation-latency-vehicle-distribution-1x}
\end{figure}

The end-to-end latency of the vehicle distribution job over a span of \SI{30}{\minute} is shown in figure~\ref{fig:evaluation-latency-vehicle-distribution-1x-long}. While the regular sawtooth pattern from the shorter excerpt can be observed the whole time, its center varies more when looking when regarding this longer span. A periodic pattern with a period of several minutes emerges in all jobs when looking at higher scaling factors up to 16\texttimes. This can be seen in the first \SI{600}{\second} in figure~\ref{fig:evaluation-latency-vehicle-distribution-1x-long} and even more in figures~\ref{fig:appendix-latency-all-1x}--\ref{fig:appendix-latency-all-16x}. The periods are aligned across jobs that run simultaneously, but are not constant over time within measurements of a single scaling factor, and the period length generally increases with higher scaling factors. Still, the median end-to-end latency decreases, as shown in figure~\ref{fig:evaluation-latency-distribution-vehicle-distribution} (see figure~\ref{fig:appendix-latency-distribution-all-e2e} for all jobs). On the other hand, the tail of the ingestion-to-processing latency (effectively the Kafka transport latency) increases but still remains negligibly small. The irregularity in the periodic pattern also decreases, which can probably attributed to the decrease in variance of event time since event timestamps are adjusted to be within \SI{0.1}{\second} of the latest known event timestamp for volume scaling. This is less variance than occurs naturally, as can be inferred from figure~\ref{fig:usecase-hsl-ingestion-lag}, but is hard to accurately replicate for replays.

\begin{figure}
	\centering
   \includegraphics[width=\textwidth]{plot_latency_distribution_delay_distribution}
	\caption{End-to-end latency distribution for vehicle distribution job at different scaling factors}
	\label{fig:evaluation-latency-distribution-vehicle-distribution}
\end{figure}

\begin{figure}
	\centering
   \includegraphics[width=\textwidth]{plot_kafka_latencies_by_scale}
	\caption{Ingestion-to-processing latency distribution for all jobs at different scaling factors}
	\label{fig:evaluation-kafka-latency-distribution}
\end{figure}

When increasing the scaling factor beyond 16\texttimes, the measurement loses the regular pattern becomes more and more unpredictable, as can be seen in figures~\ref{fig:appendix-latency-all-32x}--\ref{fig:appendix-latency-all-64x}. At 32\texttimes the normal volume, the CPU utilization on Flink task managers is between 20\% and 70\%, while the latency tracker is at over 90\%. At 64\texttimes, the task manager load is always below 30\%, and the latency tracker below 10\%. Additionally, the Kafka load is low and throughput is much less than expected. Therefore, the latency tracker and later the ingestion component become bottlenecks at higher scaling factors. Therefore, we regard only the measurements for 1\texttimes --16\texttimes as reliable.





\section{Discussion}
At the normal volume that can be expected from the HSL HFP API during the day, our solution can produce results in near real-time. An end-to-end latency under \SI{10}{\second} (and in some cases as low as \SI{2}{\second}) is sufficient for many applications, especially since the functional parts of the processing latency cannot be overcome without increased resource requirements (due to more frequent window evaluations or tighter watermarks resulting in more recomputations due to late data). Also, ultra-low latencies can be achieved if desirable through optimization and appropriate configuration, as has been demonstrated by others.

The results also show that the system is perfectly capable of handling up to 16\texttimes the normal volume. However, we cannot make statements about the performance at larger volumes with certainty. During all experiments, the CPU utilization of the Flink task manager nodes remained reasonably low despite their low computational power and low parallelism. Rather, we are pushing the limits of the ingestion component because it does not parallelize well, resulting in slow ingestion and therefore the event time experienced at the processing component is slowed down (i.e. the event-time skew increases). Additionally, the latency tracker cannot handle the large volume and therefore produces unreliable results. But even at low volumes, we are not certain of the accuracy of our results, since there are many factors that can distort the measurements such as the approximation of latencies.

In many real-world scenarios, however, the ingestion can be scaled much better and therefore our solution can still successfully be applied. Operator throughput and Kafka consumer lag are important metrics when monitoring the real-time capability of a stream processing system. These can be captured through Flink's metric system, which can replace our custom latency tracker in production scenarios (see~\cite{Apache.2019b}). This also enables monitoring of other features like checkpointing and memory used for keeping state. We found that the recommended state backend RocksDB severely slows down our jobs, and prevents the emergency detection jobs from making progress when checkpointing is disabled (pattern recognition has large state). Since RocksDB is recommended for production because it writes state to disk, this needs to be investigated.
