Understanding the challenges that are inherent to the building blocks of stream analytic is key to building a good solution.



\section{Batch Processing}
Historically, data was processed in form of bounded batch datasets. A batch processing system takes a large amount of input data and runs a \textit{job} to process it. The produced result are often analytics, but arbitrary applications like search index building and machine learning feature extraction can be processed with this method. Since batch jobs usually take a while to execute, they are not interactive but scheduled to run periodically. For example, web server logs can be imported once per day from the web server \glspl{node} and then processed. Throughput, i.e. the amount of data processed per second, is a key metric since data volume is usually very large~\cite[p.~390]{Kleppmann.2017}.

As the volume of data grew, it became too large to be handled by a single \gls{node} (we use the term \textit{node} to refer to an individual server in a cluster). This sparked the development of distributed processing engines like Hadoop~\cite{Apache.2020} (based on the MapReduce~\cite{Dean.2008} programming model) and Spark~\cite{Apache.2020b}. These frameworks tackle two common challenges of large-scale batch processing~\cites[p.~429]{Kleppmann.2017}[pp.~362--373]{Akidau.2018}:
\begin{itemize}
	\item Scalability: support for distributed processing across nodes requires orchestration and \textit{partitioning}, i.e. the division of the data set into subsets that can be processed in parallel, possibly on different \glspl{node}
	\item Fault-tolerance: guarantee of consistent and correct results even in case of job failures caused, for example, by hardware failure or scheduler-induced preemption
\end{itemize}
Having a framework to handle these issues makes focusing on the actual problem much easier.

Distributed batch processing engines assume that all functions applied to the data are stateless (no intermediate results are stored) and have no externally visible side effects (e.g., database updates)~\cite[p.~430]{Kleppmann.2017}. While these assumptions result in a deliberately restricted programming model, they facilitate distributed execution. Since no state needs to be shared between nodes, partition-based scalability is simple. In case of faults, the job can be restarted using the same input data, and the final output will be the same as if no faults had occurred (assuming deterministic operations). This is possible because input data are stored in a distributed and fault-tolerant file system like HDFS~\cite{Apache.2020}. Therefore, the file system facilitates processing across multiple nodes.

Batch processing has been successfully applied at massive scales, with Hadoop clusters at Yahoo of 35,000 nodes being used to store \SI{600}{\peta\byte} of data and run 34 million jobs every month~\cite{YahooDeveloperNetwork.2020}. However, it is only suitable for applications where low latencies are not required. Batch engines fall short when real-time processing is required, since they only process data once all input data are available. In practice, most data arrive as a continuous stream but need to be divided into batches of a certain size for processing~\cite[p.~439]{Kleppmann.2017}. An obvious solution might be to decrease the batch size and run the job at a higher frequency, a technique known as micro-batching. This can decrease the latency to a few seconds, but ultra-low latency applications are still infeasible with micro-batch processing. This is especially true when considering that data might arrive with a delay, which usually requires deferred processing or re-proccessing when the late data arrive. Also, jobs that might span batch bounds, such as user session analysis in web applications, are inherently complex to implement~\cite[pp.~34--35]{Akidau.2018}.

Apart from the technical shortcomings, processing a continous stream of data in batches seems wrong from a philosohphical point of view. Batch processing frameworks are fundamentally ill-suited for this type of data. Why not build processing engines specifically designed with continuous data streams in mind, that can overcome and embrace stream characteristics to enable new types of applications?



\section{Stream Processing}
Stream-native processing, as opposed to batch processing on streams, comes with many challenges, but is ultimately the more powerful approach when dealing with continous data streams. This section is an introduction to streams and stream processing, showing the fundamental characteristics and challenges.


\subsection{Streaming Data Properties}
properties: unbounded, unordered, varying event time skew
stream vs table

streams are natural for many data
even many batch processing applications only use batch because of a lack of digitalization~\cite[p.~29]{Kreps.2014}
batch is easy to reason about completeness since it is bounded

concepts of event time and processing time
analysis of data based on when they are observed as opposed to when they occur is usually not sufficient
need to handle event time separately

event time and processing time often do not coincide
example: delayed data (vehicle in tunnel, phones in airplane), fast-forward through historic data


\subsection{Challenges}
define 4 attributes


\subsection{Stream Processing Architectures}
Storm was first widespread stream system but tradedoff correctness for latency
use lambda for correctness at first
spark streaming as first large-scale stream processing engine with correctness guarantee for kappa architecture, nut only processing time

correctness needed for stream to get parity with batch, time gets you beyond batch~\cite[p.~28]{Akidau.2018}

instead of chopping up natural streams as in batch processing, embrace characteristics (unbounded) and process stream continuously
achieves much lower latency
throughput may suffer, but future developments might help

\begin{quote}
	We propose that a fundamental shift of approach is necessary to deal with these evolved requirements in modern data processing. We as a field must stop trying to groom unbounded datasets into finite pools of information that eventually become complete, and instead live and breathe under the assumption that we will never know if or when we have seen all of our data, only that new data will arrive [and] old data may be retracted.~\cite[p.~1792]{TylerAkidau.2015}
\end{quote}

batch can be processed with streaming system
explanation of a true streaming use case~\cite[p.~386]{Akidau.2018}


\subsection{Processing Patterns}
stream processing patterns~\cite[p.~35]{Akidau.2018}:
\begin{description}
	\item[Time-Agnostic] very simple because no reasoning about time, only logic-based on single record, like filtering or inner join
	\item[Approximation] complicated algorithms like streaming k-means, some with provable error bounds
	\item[Windowing] chopping up stream into bounded datasets, but not necessarily with fixed bounds like in batch but allow arbitrary windows (like session)
\end{description}

windowing is what we call stream analytics
relates multiple elements using time as ordering
relation requires keeping state


\subsection{Timely Processing}
windowing as division of unbounded stream in bounded segments
can be arbitrary, but usually time or count
will focus on time, since count is effectively processing time
fixed, sliding, session, fixed is special case of sliding window
custom window using window assigner

triggers define result materialization in processing time
repeated or based on watermarks as measure of completeness
\begin{quote}Repeated update triggers are great for use cases in which we simply want periodic updates to our results over time and are fine with those updates converging toward correctness with no clear indication of when correctness is achieved.~\cite[p.~63]{Akidau.2018}\end{quote}

watermarks as heuristic, show different options
difference between watermark and allowed lateness
bounded out of orderness, ascending...

result refinement mode when having multiple triggers (fire and purge)

processing time is natural, event time requires special techniques
need to define measure of completeness to maximize correctness
different methods for handling late data: retract old results, separate output, dismiss
tradeoff completeness vs latency

based on processing time: simple and perfect measure of completeness, applicable in many cases where observation time is desired

based on event time: required when event time is desired, requires more buffering than processing time, usually no perfect measure of completeness, therefore based on heuristic


\subsection{Stateful Processing}
\cite[Chapter~7]{Akidau.2018}
Many applications, especially as complex ones as analytics require state
e.g. partial matches or intermediate results of aggregations

for batch: assumed that job can be restarted completely when it fails
for streaming: assume that data might not be replayable from beginning, correctness and efficiency require persistent state

exactly once guarantees for correctness, requires offset and replayable source (at least data since last checkpoint)
requires explicit state which is known to cluster and can be checkpointed
exactly once especially important when side effects are non-idempotent~\cite[Chapter~5]{Akidau.2018}



\section{Stream Transport}
message queue, often ephemeral
plain socket stream
pubsub


\subsection{Immutable logs}
append-only immutable log with persistency

Reasons~\cite[p.~31]{Kreps.2014}
\begin{itemize}
	\item flexible consumers, also for debugging
	\item ordering
	\item Buffering and isolation, e.g. for backpressure handling and replay on node failure, important prerequisite for robustness and correctness
\end{itemize}




\section{Event Processing}
not based on data shape/cardinality like batch or stream
rather data element type
however, often streams of events

event types and definitions
event type vs event instance

\subsection{Event Driven Architecture}
event happens in an instant
complex events are multiple events in correlated according to a pattern (have a duration)
composite event would be more fitting, but complex event is prevailing term

event driven types
event notification
event sourcing
event-carried state transfer

geoevents


\subsection{Pattern Recognition}
Also called Complex Event Processing, but ambiguous
pattern recognition performed on event streams
seit sql:2016 auch iso standard
not bound to stream processing, also e.g. microservices

selection of events to evaluate by window or consumption mode
